{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Anomaly Detection on MNIST\n",
    "\n",
    "This notebook shows how a Deep Learning Auto-Encoder model can be used to find outliers in a dataset. \n",
    "\n",
    "Consider the following three-layer neural network with one hidden layer and the same number of input neurons (features) as output neurons.  The loss function is the MSE between the input and the output.  Hence, the network is forced to learn the identity via a nonlinear, reduced representation of the original data.  Such an algorithm is called a deep autoencoder; these models have been used extensively for unsupervised, layer-wise pretraining of supervised deep learning tasks, but here we consider the autoencoder's application for discovering anomalies in data.\n",
    "\n",
    "We use the well-known MNIST dataset of hand-written digits, where each row contains the 28^2=784 raw gray-scale pixel values from 0 to 255 of the digitized digits (0 to 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Theano/Lasagne and the MNIST training/testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import lasagne\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the MNIST digits dataset (actually, these are already downloaded locally)\n",
    "# !wget -N --directory-prefix=./data/MNIST/ http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "# Load training and test splits as numpy arrays\n",
    "train, val, test = pickle.load(gzip.open('data/MNIST/mnist.pkl.gz'), encoding='iso-8859-1')\n",
    "\n",
    "X_train, y_train = train\n",
    "# Omit the validation set...\n",
    "X_test, y_test = test\n",
    "\n",
    "#X_train[1000][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For training, we want to sample examples at random in small batches - don't care about the 'y_target'\n",
    "def batch_gen(X, N): \n",
    "    while True:\n",
    "        idx = np.random.choice(len(X), N)\n",
    "        yield X[idx].astype('float32')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding outliers - ugly hand-written digits\n",
    "\n",
    "We train a Deep Learning Auto-Encoder to learn a compressed (low-dimensional) non-linear representation of the dataset, hence learning the intrinsic structure of the training dataset. The auto-encoder model is then used to transform all test set images to their reconstructed images, by passing through the lower-dimensional neural network. We then find outliers in a test dataset by comparing the reconstruction of each scanned digit with its original pixel values. The idea is that a high reconstruction error of a digit indicates that the test set point doesn't conform to the structure of the training data and can hence be called an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn what's normal from the training data\n",
    "\n",
    "Train unsupervised Deep Learning autoencoder model on the training dataset. For simplicity, we train a model with 1 hidden layer of 50 Tanh neurons to create 50 non-linear features with which to reconstruct the original dataset.  For now, please accept that 50 hidden units is a reasonable choice... \n",
    "\n",
    "For simplicity, we train the auto-encoder for only 5 epoch (fives passes over the entire traing dataset).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A very simple network, an autoencoder with a single hidden layer of 50 neurons\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, 784))\n",
    "l_hidden = lasagne.layers.DenseLayer(l_in,\n",
    "                                    num_units=50,\n",
    "                                    nonlinearity=lasagne.nonlinearities.tanh)\n",
    "l_out = lasagne.layers.DenseLayer(l_hidden,\n",
    "                                    num_units=784,\n",
    "                                    nonlinearity=lasagne.nonlinearities.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Symbolic variable for our input features\n",
    "X_sym = theano.tensor.matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano expressions for the output distribution and loss vs the original input\n",
    "output = lasagne.layers.get_output(l_out, X_sym)\n",
    "\n",
    "# The loss function is the sum-squared-error averaged over a minibatch\n",
    "sample_loss = theano.tensor.mean(lasagne.objectives.squared_error(output, X_sym), axis=1)\n",
    "minibatch_loss = theano.tensor.mean(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We retrieve all the trainable parameters in our network\n",
    "params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "# Compute Adam updates for training (scores on right show training speed variation)\n",
    "updates = lasagne.updates.adam(minibatch_loss, params)      # 0.065 ... 0.032\n",
    "#updates = lasagne.updates.adagrad(minibatch_loss, params)   # 0.056 ... 0.037\n",
    "#updates = lasagne.updates.rmsprop(minibatch_loss, params)   # 0.059 ... 0.041\n",
    "#updates = lasagne.updates.adadelta(minibatch_loss, params)  # 0.101 ... 0.065\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We define a training function that will compute the loss, and take a single optimization step\n",
    "f_train = theano.function([X_sym], minibatch_loss, updates=updates)\n",
    "\n",
    "# The prediction function doesn't require targets, and outputs only the autoencoder loss for the sample\n",
    "f_predict = theano.function([X_sym], [output, sample_loss])\n",
    "\n",
    "print(\"Theano functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll choose a batch size, and calculate the number of batches in an \"epoch\"\n",
    "BATCH_SIZE = 64\n",
    "N_BATCHES = len(X_train) // BATCH_SIZE\n",
    "\n",
    "# Minibatch generators for the training and validation sets\n",
    "train_batches = batch_gen(X_train, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB:  Each epoch should take 10-20 seconds, although the first one may take a little longer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    train_loss = 0\n",
    "    for _ in range(N_BATCHES):\n",
    "        X = next(train_batches)\n",
    "        loss  = f_train(X)\n",
    "        train_loss += loss\n",
    "    train_loss /= N_BATCHES\n",
    "    print('Epoch {:2d}, Train loss {:.03f}'.format( epoch, train_loss, ))\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers in the test data\n",
    "\n",
    "The Anomaly app computes the per-row reconstruction error for the test data set. It passes it through the autoencoder model (built on the training data) and computes mean square error (MSE) for each row in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_reconstructed, test_loss = f_predict(X_test)\n",
    "test_loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the good, the bad and the *ugly*\n",
    "\n",
    "We will need a helper function for plotting handwritten digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_by_index(X, indices):\n",
    "    plt.figure(figsize=(12,3))\n",
    "    for i in range(len(indices)):\n",
    "        plt.subplot(1, 12, i+1)\n",
    "        plt.imshow(X[indices[i]].reshape((28, 28)), cmap='gray', interpolation='nearest')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the test set points with low/median/high reconstruction errors. We will now visualize the original test set points and their reconstructions obtained by propagating them through the narrow neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the test set into recostruction error order\n",
    "test_loss_sorted_indices = np.argsort( test_loss )\n",
    "\n",
    "# Here are the best ones\n",
    "test_loss[ test_loss_sorted_indices[0:10] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The good\n",
    "\n",
    "Let's plot the 12 digits with lowest reconstruction error. First we plot the reconstruction, then the original scanned images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = test_loss_sorted_indices[0:12]\n",
    "\n",
    "plot_by_index(X_test, indices)\n",
    "plot_by_index(test_reconstructed, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, a well-written digit 1 appears in both the training and testing set, and is easy to reconstruct by the autoencoder with minimal reconstruction error. Nothing is as easy as a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bad\n",
    "\n",
    "Now let's look at the 12 digits with median reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mid = len(test_loss_sorted_indices)//2\n",
    "indices = test_loss_sorted_indices[mid-6:mid+6]\n",
    "\n",
    "plot_by_index(X_test, indices)\n",
    "plot_by_index(test_reconstructed, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These test set digits look \"normal\" - it is plausible that they resemble digits from the training data to a large extent, but they do have some particularities that cause some reconstruction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ugly\n",
    "\n",
    "And here are the biggest outliers - The 12 digits with highest reconstruction error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = test_loss_sorted_indices[-12:]\n",
    "\n",
    "plot_by_index(X_test, indices)\n",
    "plot_by_index(test_reconstructed, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here are some pretty ugly digits that are plausibly not commonly found in the training data - some are even hard to classify by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voila!\n",
    "\n",
    "We were able to find outliers with Deep Learning Auto-Encoder models.  \n",
    "\n",
    "We would love to hear your use-case for Anomaly detection...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References\n",
    "\n",
    "See : \n",
    "    \n",
    "*  https://github.com/h2oai/h2o-training-book/blob/master/hands-on_training/anomaly_detection.md\n",
    "   *   h2o-training-book/package.json :: \"license\": \"Apache 2\",\n",
    "\n",
    "*  http://goelhardik.github.io/2016/06/04/mnist-autoencoder/\n",
    "\n",
    "*  https://github.com/mikesj-public/convolutional_autoencoder/blob/master/mnist_conv_autoencode.ipynb\n",
    "\n",
    "*  https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises\n",
    "\n",
    "*  What is the test set error *before* network training?\n",
    "\n",
    "*  Check whether 20 hidden units or 100 hidden units would have been a better choice\n",
    "\n",
    "*  See how the learning progresses using ```adadelta``` updates\n",
    "\n",
    "*  Try adding your own example digits to ```./images/mnist/``` - there's a template ```.png``` file there to start - and have a look at the reconstruction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = plt.imread('./images/mnist/template_28x28.png')\n",
    "plt.imshow(im, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_dir = './images/mnist/'\n",
    "\n",
    "image_files = [ '%s/%s' % (image_dir, f) for f in os.listdir(image_dir) \n",
    "                 if (f.lower().endswith('png') or f.lower().endswith('jpg')) ]\n",
    "\n",
    "v=[]\n",
    "for i, f in enumerate(image_files):\n",
    "    im = plt.imread(f)\n",
    "    #print(\"Image File:%s\" % (f,))\n",
    "    v.append( im.flatten() )\n",
    "\n",
    "# v=[ plt.imread(f).flatten() for f in image_files ]\n",
    "v_reconstructed, v_loss = f_predict(v)\n",
    "\n",
    "v_all_indices = np.arange(len(v))\n",
    "\n",
    "plot_by_index(v, v_all_indices)\n",
    "plot_by_index(v_reconstructed, v_all_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}