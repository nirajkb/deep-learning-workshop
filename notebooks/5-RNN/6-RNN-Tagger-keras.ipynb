{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tagger\n",
    "\n",
    "This example trains a RNN to tag words from a corpus - \n",
    "\n",
    "The data used for training is from a Wikipedia download, which is the artificially annotated with parts of speech by the NLTK PoS tagger written by Matthew Honnibal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "SENTENCE_LENGTH_MAX = 32\n",
    "EMBEDDING_DIM=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text and Parsing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a Wikipedia Corpus\n",
    "\n",
    "From the corpus download page : http://wortschatz.uni-leipzig.de/en/download/\n",
    "\n",
    "Here's the paper that explains how the corpus was constructed : \n",
    "\n",
    "*  D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n",
    "    *  In: Proceedings of the 8th International Language Ressources and Evaluation (LREC'12), 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_dir = './data/RNN/'\n",
    "corpus_text_file = os.path.join(corpus_dir, 'en.wikipedia.2010.100K.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile( corpus_text_file ):\n",
    "    raise RuntimeError(\"You need to download the corpus file : \"+\n",
    "                       \"Use the downloader in 5-Text-Corpus-and-Embeddings.ipynb\")\n",
    "else:\n",
    "    print(\"Corpus available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_sentence_tokens(corpus_text_file=corpus_text_file):\n",
    "    while True:\n",
    "        with open(corpus_text_file, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                n,l = line.split('\\t')   # Strip of the initial numbers\n",
    "                for s in sentence_splitter.tokenize(l):  # Split the lines into sentences (~1 each)\n",
    "                    tree_banked = tokenizer.tokenize(s)\n",
    "                    if len(tree_banked) < SENTENCE_LENGTH_MAX:\n",
    "                        yield tree_banked\n",
    "        print(\"Corpus : Looping\")\n",
    "corpus_sentence_tokens_gen = corpus_sentence_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "' | '.join(next(corpus_sentence_tokens_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "pos_tagger = PerceptronTagger(load=True)\n",
    "' | '.join(list(pos_tagger.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Let 's see what part of speech analysis on Jeff 's sample text looks like .\".split(' ')\n",
    "#s = next(corpus_sentence_tokens_gen)\n",
    "pos_tagger.tag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twist : Not interested in all classes...\n",
    "\n",
    "To simplify (dramatically), our RNN will be trained to just tell the difference between 'is ordinary word' and 'is entity name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = 'O E'.split(' ')\n",
    "pos_tagger_entity_tags = set('NNP'.split(' '))\n",
    "pos_tagger_to_idx   = dict([ (t,(1 if t in pos_tagger_entity_tags else 0)) \n",
    "                            for i,t in enumerate(pos_tagger.classes)])\n",
    "TAG_SET_SIZE= len(tag_list)\n",
    "\n",
    "pos_tagger_to_idx['NNP'], pos_tagger_to_idx['VBP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = './data/RNN/'\n",
    "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
    "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
    "\n",
    "if not os.path.isfile( glove_100k_50d_path ):\n",
    "    raise RuntimeError(\"You need to download GloVE Embeddings \"+\n",
    "                       \": Use the downloader in 5-Text-Corpus-and-Embeddings.ipynb\")\n",
    "else:\n",
    "    print(\"GloVE available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)\n",
    "import glove\n",
    "word_embedding = glove.Glove.load_stanford( glove_100k_50d_path )\n",
    "word_embedding.word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An RNN Part-of-Speech Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "RNN_HIDDEN_SIZE = EMBEDDING_DIM # ?+1 for capitalisation flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the Embedding  Keras-Compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding.word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_rnn = np.vstack([ \n",
    "        np.zeros( (1, EMBEDDING_DIM,), dtype='float32'),   # This is the 'zero' value (used as a mask in Keras)\n",
    "        np.zeros( (1, EMBEDDING_DIM,), dtype='float32'),   # This is for 'UNK'  (word == 1)\n",
    "        word_embedding.word_vectors,\n",
    "    ])\n",
    "word_embedding_rnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesising a 'correct answer' for the Tagger\n",
    "\n",
    "Normally, this would be the (manual) annotations from the corpus itself.  However, we don't have an annotated corpus.  Instead, we're going to use the annotations produced by the NTLK tagger - simplified to only identify 'NNP = entities'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_idx_rnn(word):\n",
    "    idx = word_embedding.dictionary.get(word.lower(), -1)  # since UNK=1 = (-1+2)\n",
    "    return idx+2  # skip ahead 2 places\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.utils.np_utils import to_categorical\n",
    "\n",
    "def sentences_for_network(list_of_sentences, include_targets=False, one_hot_targets=False):\n",
    "    len_of_list = len(list_of_sentences)\n",
    "    #print(\"sentences_for_network.sentences.length = %d\" % (len_of_list,))\n",
    "    \n",
    "    input_values = np.zeros((len_of_list, SENTENCE_LENGTH_MAX), dtype='int32')\n",
    "    for i, sent in enumerate(list_of_sentences):\n",
    "        for j, word in enumerate(sent):\n",
    "            input_values[i,j] = word_to_idx_rnn(word)\n",
    "    \n",
    "    if not include_targets: \n",
    "        return (input_values, None)\n",
    "\n",
    "    if one_hot_targets:\n",
    "        # Add extra dimension here to suit Keras' TimeDistributed(Dense(softmax))\n",
    "        #   as discussed : https://github.com/fchollet/keras/issues/6363\n",
    "        target_values  = np.zeros((len_of_list, SENTENCE_LENGTH_MAX, TAG_SET_SIZE), dtype='int32')\n",
    "    else:\n",
    "        target_values  = np.zeros((len_of_list, SENTENCE_LENGTH_MAX), dtype='int32')\n",
    "        \n",
    "    for i, sent in enumerate(list_of_sentences):\n",
    "        sentence_tags = pos_tagger.tag(sent)\n",
    "        for j, word_tag in enumerate(sentence_tags):\n",
    "            tag = word_tag[1] # tags are returned as tuples (word, tag)\n",
    "            pos_class = pos_tagger_to_idx[tag]  # These are the class #s\n",
    "            if one_hot_targets:\n",
    "                target_values[i,j] = to_categorical(pos_class, num_classes=TAG_SET_SIZE)\n",
    "            else:\n",
    "                target_values[i,j] = pos_class\n",
    "\n",
    "    return (input_values, target_values)\n",
    "\n",
    "def batch_for_network_generator():\n",
    "    while True:\n",
    "        batch_of_sentences = [ next(corpus_sentence_tokens_gen) for i in range(BATCH_SIZE) ]    \n",
    "        yield sentences_for_network(batch_of_sentences, include_targets=True, one_hot_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the batchifier\n",
    "\n",
    "This just finds the next values to be produced - it isn't needed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch_input, single_batch_targets = next(batch_for_network_generator())\n",
    "single_batch_input.shape, single_batch_targets.shape\n",
    "#single_batch_input[0]\n",
    "#single_batch_targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN Symbolically\n",
    "\n",
    "#### Good blog post series\n",
    "\n",
    "*  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\n",
    "\n",
    "#### Keras Examples\n",
    "\n",
    "*  https://github.com/fchollet/keras/issues/5022 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.contrib.keras.api.keras.preprocessing import sequence\n",
    "from tensorflow.contrib.keras.api.keras.layers import Input, Embedding, GRU, Dense #, Activation\n",
    "from tensorflow.contrib.keras.api.keras.models import Model\n",
    "\n",
    "# Hmm : The following is not in the API...\n",
    "from tensorflow.contrib.keras.python.keras.layers import Bidirectional, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_input = Input(shape=(SENTENCE_LENGTH_MAX,), dtype='int32', name=\"SentencesTokens\")\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "#   note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedded_sequences = Embedding(word_embedding_rnn.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[ word_embedding_rnn ],\n",
    "                                input_length=SENTENCE_LENGTH_MAX,\n",
    "                                trainable=False, \n",
    "                                mask_zero=True,\n",
    "                                name=\"SentencesEmbedded\") (tokens_input)\n",
    "\n",
    "#extra_input = ...\n",
    "aggregate_vectors = embedded_sequences # concat...\n",
    "\n",
    "rnn_outputs = Bidirectional( GRU(RNN_HIDDEN_SIZE, return_sequences=True),  merge_mode='concat' )(aggregate_vectors)\n",
    "\n",
    "is_ner_outputs  = TimeDistributed( Dense(TAG_SET_SIZE, activation='softmax'), \n",
    "                                   input_shape=(BATCH_SIZE, SENTENCE_LENGTH_MAX, RNN_HIDDEN_SIZE*2),\n",
    "                                   name='POS-class')(rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[tokens_input], outputs=[is_ner_outputs])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")  # , metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase for the RNN\n",
    "\n",
    "This will actually **train** the RNN - which can take 3-5minutes (depending on your CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.fit(x, y_one_hot)\n",
    "model.fit_generator(batch_for_network_generator(), 1000, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the learned parameters\n",
    "\n",
    "Save the model weights to disk if there isn't a version there already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_file = './data/cache/tagger_rnn_trained_keras.h5'\n",
    "\n",
    "# Actually, this includes the embedding, which is a little redundant\n",
    "if not os.path.isfile( weights_file ):\n",
    "    model.save_weights(weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights into network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.isfile( weights_file ):\n",
    "    model.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the Tagger Network 'works'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_results_for(test_sentences):\n",
    "    #sentences_for_network(list_of_sentences, include_targets=False, one_hot_targets=False)\n",
    "    input_values, target_values_int = sentences_for_network(test_sentences, include_targets=True)\n",
    "\n",
    "    rnn_output = model.predict_on_batch(input_values)\n",
    "\n",
    "    # rnn_output here is a softmax-vector at every word location\n",
    "    for i,sent in enumerate(test_sentences): # [0:5]):\n",
    "        annotated = [ \n",
    "                \"%s-%d-%d\" % (word, target_values_int[i,j], np.argmax(rnn_output[i,j]), )    \n",
    "                for j,word in enumerate(sent) \n",
    "            ]\n",
    "        print(' '.join(annotated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[\n",
    "    \"Dr. Andrews works at Red Cat Labs .\",\n",
    "    \"Let 's see what part of speech analysis looks like .\",\n",
    "    \"When are you off to New York , Chaitanya ?\",\n",
    "]\n",
    "\n",
    "# Uncomment this for 8 sentences from the corpus\n",
    "#sentences = [ ' '.join(next(corpus_sentence_tokens_gen)) for i in range(8) ]\n",
    "\n",
    "test_sentences_mixed = [ s.split(' ') for s in sentences ]\n",
    "test_sentences_title = [ s.title().split(' ') for s in sentences ]\n",
    "test_sentences_single = [ s.lower().split(' ') for s in sentences ]\n",
    "#test_sentences_single = [ s.upper().split(' ') for s in sentences ]\n",
    "\n",
    "print(\"Format : WORD-NLTK-RNN\\n\")\n",
    "\n",
    "tag_results_for(test_sentences_mixed)\n",
    "print()\n",
    "tag_results_for(test_sentences_title)\n",
    "print()\n",
    "tag_results_for(test_sentences_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  And let's look at the Statistics\n",
    "\n",
    "... actually, looking at the above samples, the NLTK PoS tagger is HOPELESS when the text is converted to a single case, or title case. QED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1.  Make the tagger identify different PoS (say : 'verbs')\n",
    "\n",
    "2.  Make the tagger return several different tags instead\n",
    "\n",
    "3.  See whether more advanced 'LSTM' nodes would improve the scores\n",
    "\n",
    "4.  Add a special 'is_uppercase' element to the embedding vector (or, more simply, just replace one of the elements with an indicator).  Does this help the NNP accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}