{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Corpus and Embeddings\n",
    "\n",
    "This example trains a RNN to tag words from a corpus - \n",
    "\n",
    "The data used for training is from a Wikipedia download, which is the artificially annotated with parts of speech by the NLTK PoS tagger written by Matthew Honnibal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "SENTENCE_LENGTH_MAX = 32\n",
    "EMBEDDING_DIM=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text and Parsing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentence_splitter.tokenize(\"This is Mr. Smith's tokenized test. The U.S.A gives us sent two. Is this sent three?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"This is Mr. Smith's tokenized test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a Wikipedia Corpus\n",
    "\n",
    "From the corpus download page : http://wortschatz.uni-leipzig.de/en/download/\n",
    "\n",
    "Here's the paper that explains how the corpus was constructed : \n",
    "\n",
    "*  D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n",
    "    *  In: Proceedings of the 8th International Language Ressources and Evaluation (LREC'12), 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_dir = './data/RNN/'\n",
    "corpus_text_file = os.path.join(corpus_dir, 'en.wikipedia.2010.100K.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile( corpus_text_file ):\n",
    "    if not os.path.exists(corpus_dir):\n",
    "        os.makedirs(corpus_dir)\n",
    "\n",
    "    corpus_text_tar = 'eng_wikipedia_2010_100K.tar.gz'    \n",
    "    download_url = 'http://pcai056.informatik.uni-leipzig.de/downloads/corpora/'+corpus_text_tar\n",
    "\n",
    "    data_cache = './data/cache'\n",
    "    if not os.path.exists(data_cache):\n",
    "        os.makedirs(data_cache)\n",
    "    \n",
    "    # Fall-back url if too slow\n",
    "    #download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+corpus_text_tar\n",
    "\n",
    "    import shutil, requests\n",
    "\n",
    "    # Get the download path from the web-service\n",
    "    #urllib.request.urlretrieve('http://wortschatz.uni-leipzig.de/download/service', corpus_text_tar)\n",
    "    # download_url = ...\n",
    "    \n",
    "    tarfilepath = os.path.join(data_cache, corpus_text_tar)\n",
    "    if not os.path.isfile( tarfilepath ):\n",
    "        response = requests.get(download_url, stream=True)\n",
    "        with open(tarfilepath, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "    if os.path.isfile(tarfilepath):\n",
    "        import tarfile\n",
    "        #tarfile.open(tarfilepath, 'r:gz').extractall(corpus_dir)\n",
    "        tarfile.open(tarfilepath, 'r:gz').extract('eng_wikipedia_2010_100K-sentences.txt', corpus_dir)\n",
    "    shutil.move(os.path.join(corpus_dir, 'eng_wikipedia_2010_100K-sentences.txt'), corpus_text_file)\n",
    "    \n",
    "    # Get rid of tarfile source (the required text file itself will remain)\n",
    "    #os.unlink(tarfilepath)\n",
    "\n",
    "print(\"Corpus available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This is a work-in-progress, since we should really discover 'download_url' from the 'service'\n",
    "#r=requests.post('http://wortschatz.uni-leipzig.de/download/service', data='file=%s&func=\"link\"' % (corpus_text_tar,))\n",
    "#r=requests.post('http://wortschatz.uni-leipzig.de/download/service', data=dict(file=corpus_text_tar, func=\"link\") )\n",
    "#r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_sentence_tokens(corpus_text_file=corpus_text_file):\n",
    "    while True:\n",
    "        with open(corpus_text_file, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                n,l = line.split('\\t')   # Strip of the initial numbers\n",
    "                for s in sentence_splitter.tokenize(l):  # Split the lines into sentences (~1 each)\n",
    "                    tree_banked = tokenizer.tokenize(s)\n",
    "                    if len(tree_banked) < SENTENCE_LENGTH_MAX:\n",
    "                        yield tree_banked\n",
    "        print(\"Corpus : Looping\")\n",
    "corpus_sentence_tokens_gen = corpus_sentence_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' | '.join(next(corpus_sentence_tokens_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Word Embeddings\n",
    "Using the python package :  https://github.com/maciejkula/glove-python , and code samples from : http://developers.lyst.com/2014/11/11/word-embeddings-for-fashion/\n",
    "\n",
    "### Create the Co-occurrence Matrix\n",
    "For speed, this looks at the first 100,000 tokens in the corpus - and should create the co-occurences in 30 seconds or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glove\n",
    "glove_corpus = glove.Corpus()\n",
    "\n",
    "corpus_sentences = [ \n",
    "        [ w.lower() for w in next(corpus_sentence_tokens_gen)] # All lower-case\n",
    "        for _ in range(0,100*1000) \n",
    "    ]\n",
    "\n",
    "# Fit the co-occurrence matrix using a sliding window of 10 words.\n",
    "t0 = time.time()\n",
    "glove_corpus.fit(corpus_sentences, window=10)\n",
    "\n",
    "print(\"Dictionary length=%d\" % (len(glove_corpus.dictionary),))\n",
    "print(\"Co-occurrence calculated in %5.1fsec\" % (time.time()-t0, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the index of the word in the dictionary\n",
    "glove_corpus.dictionary['city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the Word Embedding\n",
    "\n",
    "This will make use of up to 4 threads - and each epoch takes 20-30 seconds on a single core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = glove.Glove(no_components=EMBEDDING_DIM, learning_rate=0.05)\n",
    "\n",
    "t0 = time.time()\n",
    "glove_epochs, glove_threads = 20, 4 \n",
    "\n",
    "word_embedding.fit(glove_corpus.matrix, epochs=glove_epochs, no_threads=glove_threads, verbose=True)\n",
    "\n",
    "print(\"%d-d word-embedding created in %5.1fsec = %5.1fsec per epoch\" % (\n",
    "        EMBEDDING_DIM, (time.time()-t0), (time.time()-t0)/glove_epochs*glove_threads, ))\n",
    "\n",
    "# Add the word -> id dictionary to the model to allow similarity queries.\n",
    "word_embedding.add_dictionary(glove_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_embedding.save(\"./data/RNN/glove.embedding.50.pkl\")\n",
    "#word_embedding.load(\"./data/RNN/glove.embedding.50.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word-similarity test\n",
    "word_embedding.most_similar('country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word-analogy test\n",
    "def get_embedding_vec(word):\n",
    "    idx = word_embedding.dictionary.get(word.lower(), -1)\n",
    "    if idx<0:\n",
    "        #print(\"Missing word : '%s'\" % (word,))\n",
    "        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')  # UNK\n",
    "    return word_embedding.word_vectors[idx]\n",
    "\n",
    "def get_closest_word(vec, number=5):\n",
    "    dst = (np.dot(word_embedding.word_vectors, vec)\n",
    "                   / np.linalg.norm(word_embedding.word_vectors, axis=1)\n",
    "                   / np.linalg.norm(vec))\n",
    "    word_ids = np.argsort(-dst)\n",
    "    return [(word_embedding.inverse_dictionary[x], dst[x]) for x in word_ids[:number]\n",
    "            if x in word_embedding.inverse_dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_vec = get_embedding_vec('woman') + get_embedding_vec('king') - get_embedding_vec('man')\n",
    "get_closest_word(analogy_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_analogy(s='one two three four'):\n",
    "    (a,b,c,d) = s.split(' ')\n",
    "    analogy_vec = get_embedding_vec(b) - get_embedding_vec(a) + get_embedding_vec(c)\n",
    "    words = [ w for (w,p) in get_closest_word(analogy_vec) if w not in (a,b,c)]\n",
    "    print(\"'%s' is to '%s' as '%s' is to {%s}\" % (a,b,c,', '.join(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_analogy('man woman king queen')\n",
    "test_analogy('paris france rome italy')\n",
    "test_analogy('kitten cat puppy dog')\n",
    "test_analogy('understand understood run ran')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem : Embedding is *Poor*\n",
    "### Solution : Load a pre-trained word embedding\n",
    "\n",
    "Since the embedding we learnt above is poor, let's load a pre-trained word embedding, from a much larger corpus, trained for a much longer period.  Source of this word embedding (created from a 6 billion tokens corpus, with results as 50d vectors): http://nlp.stanford.edu/projects/glove/ \n",
    "\n",
    "NB: If you don't have the required data, and the RedCatLabs server doesn't give you the download, the loader below downloads a 823Mb file via a fairly slow connection to a server at Stanford (this can take HOURS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, shutil\n",
    "\n",
    "glove_dir = './data/RNN/'\n",
    "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
    "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
    "\n",
    "# These are temporary files if we need to download it from the original source (slow)\n",
    "data_cache = './data/cache'\n",
    "glove_full_tar = 'glove.6B.zip'\n",
    "glove_full_50d = 'glove.6B.50d.txt'\n",
    "\n",
    "#force_download_from_original=False\n",
    "download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n",
    "original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n",
    "\n",
    "if not os.path.isfile( glove_100k_50d_path ):\n",
    "    if not os.path.exists(glove_dir):\n",
    "        os.makedirs(glove_dir)\n",
    "    \n",
    "    # First, try to download a pre-prepared file directly...\n",
    "    response = requests.get(download_url, stream=True)\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n",
    "        with open(glove_100k_50d_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "    else:\n",
    "        # But, for some reason, RedCatLabs didn't give us the file directly\n",
    "        if not os.path.exists(data_cache):\n",
    "            os.makedirs(data_cache)\n",
    "        \n",
    "        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n",
    "            zipfilepath = os.path.join(data_cache, glove_full_tar)\n",
    "            if not os.path.isfile( zipfilepath ):\n",
    "                print(\"Downloading 860Mb GloVE file from Stanford\")\n",
    "                response = requests.get(download_url, stream=True)\n",
    "                with open(zipfilepath, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "            if os.path.isfile(zipfilepath):\n",
    "                print(\"Unpacking 50d GloVE file from zip\")\n",
    "                import zipfile\n",
    "                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n",
    "\n",
    "        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n",
    "            with open(glove_100k_50d_path, 'wt') as out_file:\n",
    "                print(\"Reducing 50d GloVE file to first 100k words\")\n",
    "                for i, l in enumerate(in_file.readlines()):\n",
    "                    if i>=100000: break\n",
    "                    out_file.write(l)\n",
    "    \n",
    "        # Get rid of tarfile source (the required text file itself will remain)\n",
    "        #os.unlink(zipfilepath)\n",
    "        #os.unlink(os.path.join(data_cache, glove_full_50d))\n",
    "\n",
    "print(\"GloVE available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)\n",
    "word_embedding = glove.Glove.load_stanford( glove_100k_50d_path )\n",
    "word_embedding.word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded that, play around with the similarity and analogy tests again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_analogy('man woman king queen')\n",
    "test_analogy('paris france rome italy')\n",
    "test_analogy('kitten cat puppy dog')\n",
    "test_analogy('understand understood run ran')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Embedding in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "#N = 10000 # Number of items (vocab size).\n",
    "#D = 200 # Dimensionality of the embedding.\n",
    "#embedding_var = tf.Variable(tf.random_normal([N,D]), name='word_embedding')\n",
    "\n",
    "embedding_var = tf.Variable(word_embedding.word_vectors, dtype='float32', \n",
    "                            name='word_embedding')\n",
    "\n",
    "# Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "projector_config = projector.ProjectorConfig()\n",
    "\n",
    "# You can add multiple embeddings. Here we add only one.\n",
    "embedding = projector_config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# Link this tensor to its metadata file (e.g. labels).\n",
    "LOG_DIR='../../tensorflow.logdir/'\n",
    "metadata_file = 'glove_full_50d.words.tsv'\n",
    "vocab_list = [ word_embedding.inverse_dictionary[i] \n",
    "               for i in range(len( word_embedding.inverse_dictionary )) ]\n",
    "with open(os.path.join(LOG_DIR, metadata_file), 'wt') as metadata:\n",
    "    metadata.writelines(\"%s\\n\" % w for w in vocab_list)\n",
    "    \n",
    "embedding.metadata_path = os.path.join(os.getcwd(), LOG_DIR, metadata_file)\n",
    "\n",
    "# Use the same LOG_DIR where you stored your checkpoint.\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "# The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
    "# read this file during startup.\n",
    "projector.visualize_embeddings(summary_writer, projector_config)\n",
    "\n",
    "saver = tf.train.Saver([embedding_var])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the model\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver.save(sess, os.path.join(LOG_DIR, metadata_file+'.ckpt'))\n",
    "print(\"Look at the embedding in TensorBoard : http://localhost:8081/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1.  Plot some of the embeddings on a graph (potentially apply PCA first)\n",
    "\n",
    "2.  ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}