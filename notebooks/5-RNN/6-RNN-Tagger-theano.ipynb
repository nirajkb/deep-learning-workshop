{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tagger\n",
    "\n",
    "This example trains a RNN to tag words from a corpus - \n",
    "\n",
    "The data used for training is from a Wikipedia download, which is the artificially annotated with parts of speech by the NLTK PoS tagger written by Matthew Honnibal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import lasagne\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "SENTENCE_LENGTH_MAX = 32\n",
    "EMBEDDING_DIM=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text and Parsing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a Wikipedia Corpus\n",
    "\n",
    "From the corpus download page : http://wortschatz.uni-leipzig.de/en/download/\n",
    "\n",
    "Here's the paper that explains how the corpus was constructed : \n",
    "\n",
    "*  D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n",
    "    *  In: Proceedings of the 8th International Language Ressources and Evaluation (LREC'12), 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_dir = './data/RNN/'\n",
    "corpus_text_file = os.path.join(corpus_dir, 'en.wikipedia.2010.100K.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile( corpus_text_file ):\n",
    "    raise RuntimeError(\"You need to download the corpus file : Use the downloader in 5-Text-Corpus-and-Embeddings.ipynb\")\n",
    "else:\n",
    "    print(\"Corpus available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpus_sentence_tokens(corpus_text_file=corpus_text_file):\n",
    "    while True:\n",
    "        with open(corpus_text_file, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                # print(line)\n",
    "                # .decode(\"utf-8\")\n",
    "                n,l = line.split('\\t')   # Strip of the initial numbers\n",
    "                for s in sentence_splitter.tokenize(l):  # Split the lines into sentences (~1 each)\n",
    "                    tree_banked = tokenizer.tokenize(s)\n",
    "                    if len(tree_banked) < SENTENCE_LENGTH_MAX:\n",
    "                        yield tree_banked\n",
    "        print(\"Corpus : Looping\")\n",
    "corpus_sentence_tokens_gen = corpus_sentence_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' | '.join(next(corpus_sentence_tokens_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "pos_tagger = PerceptronTagger(load=True)\n",
    "' | '.join(list(pos_tagger.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"Let 's see what part of speech analysis on this sample text looks like .\".split(' ')\n",
    "#s = next(corpus_sentence_tokens_gen)\n",
    "pos_tagger.tag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twist : Not interested in all classes...\n",
    "\n",
    "To simplify (dramatically), our RNN will be trained to just tell the difference between 'is ordinary word' and 'is entity name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_list = 'O E'.split(' ')\n",
    "pos_tagger_entity_tags = set('NNP'.split(' '))\n",
    "pos_tagger_to_idx   = dict([ (t,(1 if t in pos_tagger_entity_tags else 0)) for i,t in enumerate(pos_tagger.classes)])\n",
    "TAG_SET_SIZE= len(tag_list)\n",
    "\n",
    "pos_tagger_to_idx['NNP'], pos_tagger_to_idx['VBP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_dir = './data/RNN/'\n",
    "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
    "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
    "\n",
    "if not os.path.isfile( glove_100k_50d_path ):\n",
    "    raise RuntimeError(\"You need to download GloVE Embeddings : Use the downloader in 5-Text-Corpus-and-Embeddings.ipynb\")\n",
    "else:\n",
    "    print(\"GloVE available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)\n",
    "import glove\n",
    "word_embedding = glove.Glove.load_stanford( glove_100k_50d_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_vec(word):\n",
    "    idx = word_embedding.dictionary.get(word.lower(), -1)\n",
    "    if idx<0:\n",
    "        #print(\"Missing word : '%s'\" % (word,))\n",
    "        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')  # UNK\n",
    "    return word_embedding.word_vectors[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An RNN Part-of-Speech Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "RNN_HIDDEN_SIZE = EMBEDDING_DIM # ?+1 for capitalisation flag\n",
    "GRAD_CLIP_BOUND = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training / Testing dataset\n",
    "And a 'batch generator' function that delivers data in the right format for RNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_sentences(size=BATCH_SIZE):\n",
    "    return [ next(corpus_sentence_tokens_gen) for i in range(size) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test it out\n",
    "batch_test = lambda : batch_sentences(size=4)\n",
    "print([ ' '.join(s) for s in batch_test()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesising a 'correct answer' for the Tagger\n",
    "\n",
    "Normally, this would be the (manual) annotations from the corpus itself.  However, we don't have an annotated corpus.  Instead, we're going to use the annotations produced by the NTLK tagger - simplified to only identify 'NNP = entities'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After sampling a data batch, we transform it into a one hot feature representation with a mask\n",
    "def prep_batch_for_network(batch_of_sentences, include_targets=False):\n",
    "    sentence_max_length = np.array([ len(w) for w in batch_of_sentences ]).max()\n",
    "    \n",
    "    # translate into one-hot matrix, mask values and targets\n",
    "    input_values = np.zeros((len(batch_of_sentences), sentence_max_length, EMBEDDING_DIM), dtype='float32')\n",
    "    mask_values  = np.zeros((len(batch_of_sentences), sentence_max_length), dtype='float32')\n",
    "    \n",
    "    for i, sent in enumerate(batch_of_sentences):\n",
    "      for j, word in enumerate(sent):\n",
    "        input_values[i,j] = get_embedding_vec(word) # this is word.lower() in dictionary\n",
    "      mask_values[i, 0:len(sent) ] = 1.\n",
    "\n",
    "    if not include_targets:\n",
    "        return input_values, mask_values        \n",
    "    \n",
    "    target_values  = np.zeros((len(batch_of_sentences), sentence_max_length), dtype='int32')\n",
    "    for i, sent in enumerate(batch_of_sentences):\n",
    "        sentence_tags = pos_tagger.tag(sent)\n",
    "        for j, word_tag in enumerate(sentence_tags):\n",
    "            target_values[i,j] = pos_tagger_to_idx[word_tag[1]]  # tags are returned as tuples (word, tag)\n",
    "    \n",
    "    return input_values, mask_values, target_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the batchifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep_batch_for_network([\"Mr. Smith works at Red Cat Labs .\".split(' ')], include_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN Symbolically\n",
    "\n",
    "#### Lasagne RNN tutorial (including conventions &amp; rationale)\n",
    "\n",
    "*  http://colinraffel.com/talks/hammer2015recurrent.pdf\n",
    "\n",
    "#### Lasagne Examples\n",
    "\n",
    "*  https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/recurrent.py\n",
    "*  https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "#### Good blog post series\n",
    "\n",
    "*  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Symbolic variables for input. In addition to the usual features and target, we need a mask\n",
    "rnn_input_sym = theano.tensor.tensor3()\n",
    "rnn_mask_sym  = theano.tensor.matrix()\n",
    "\n",
    "rnn_words_target_sym = theano.tensor.imatrix() # part-of-speech generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_input = lasagne.layers.InputLayer( (None, None, RNN_HIDDEN_SIZE) )  # batch_size, sequence_len, embedding_dim\n",
    "rnn_mask  = lasagne.layers.InputLayer( (None, None, RNN_HIDDEN_SIZE) )  # batch_size, sequence_len, embedding_dim\n",
    "\n",
    "n_batch, n_time_steps, n_features = rnn_input_sym.shape\n",
    "\n",
    "rnn_layer_f = lasagne.layers.GRULayer(rnn_input,\n",
    "                num_units=RNN_HIDDEN_SIZE,\n",
    "                gradient_steps=-1,\n",
    "                grad_clipping=GRAD_CLIP_BOUND,\n",
    "                hid_init=lasagne.init.Normal(),\n",
    "                learn_init=True,\n",
    "                mask_input=rnn_mask,\n",
    "                only_return_final=False, # Need all of the output states\n",
    "            )\n",
    "\n",
    "rnn_layer_b = lasagne.layers.GRULayer(rnn_input,\n",
    "                num_units=RNN_HIDDEN_SIZE,\n",
    "                gradient_steps=-1,\n",
    "                grad_clipping=GRAD_CLIP_BOUND,\n",
    "                hid_init=lasagne.init.Normal(),\n",
    "                learn_init=True,\n",
    "                mask_input=rnn_mask,\n",
    "                only_return_final=False, # Need all of the output states\n",
    "                backwards=True,\n",
    "            )\n",
    "\n",
    "# Before the decoder layer, we need to reshape the sequence into the batch dimension,\n",
    "# so that timesteps are decoded independently.\n",
    "rnn_reshape_f = lasagne.layers.ReshapeLayer(rnn_layer_f, (-1, RNN_HIDDEN_SIZE) )\n",
    "rnn_reshape_b = lasagne.layers.ReshapeLayer(rnn_layer_b, (-1, RNN_HIDDEN_SIZE) )\n",
    "\n",
    "# Now concatenate them\n",
    "rnn_concat = lasagne.layers.ConcatLayer([rnn_reshape_f, rnn_reshape_b])\n",
    "\n",
    "# Convert them into softmax outputs\n",
    "rnn_tag_val = lasagne.layers.DenseLayer( rnn_concat, num_units=TAG_SET_SIZE, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# And reshape them, so that they are in the original batches-of-sentences shape\n",
    "rnn_out = lasagne.layers.ReshapeLayer(rnn_tag_val, (-1, n_time_steps, TAG_SET_SIZE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, the output stage - this is for the training (over all the words in the sentences)\n",
    "rnn_output = lasagne.layers.get_output(rnn_out, \n",
    "                {\n",
    "                 rnn_input: rnn_input_sym, \n",
    "                 rnn_mask: rnn_mask_sym, \n",
    "                }\n",
    "            )\n",
    "\n",
    "# We flatten the sequence into the batch dimension before calculating the loss\n",
    "def rnn_word_cross_ent(net_output, targets):\n",
    "    preds = theano.tensor.reshape(net_output, (-1, TAG_SET_SIZE))\n",
    "    targets_flat = theano.tensor.flatten(targets)\n",
    "    cost = theano.tensor.nnet.categorical_crossentropy(preds, targets_flat)\n",
    "    return cost\n",
    "\n",
    "rnn_loss = rnn_word_cross_ent(rnn_output, rnn_words_target_sym).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and the Training and Prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For stability during training, gradients are clipped and a total gradient norm constraint can also be\n",
    "#MAX_GRAD_NORM = 15\n",
    "MAX_GRAD_NORM = None\n",
    "\n",
    "rnn_params = lasagne.layers.get_all_params(rnn_out, trainable=True)\n",
    "\n",
    "rnn_grads = theano.tensor.grad(rnn_loss, rnn_params)\n",
    "rnn_grads = [theano.tensor.clip(g, -GRAD_CLIP_BOUND, GRAD_CLIP_BOUND) for g in rnn_grads]\n",
    "if MAX_GRAD_NORM is not None:\n",
    "    rnn_grads, rnn_norm = lasagne.updates.total_norm_constraint( rnn_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "rnn_updates = lasagne.updates.adam(rnn_grads, rnn_params)\n",
    "\n",
    "rnn_train = theano.function([rnn_input_sym, rnn_words_target_sym, rnn_mask_sym],\n",
    "                [rnn_loss],\n",
    "                updates=rnn_updates,\n",
    "            )\n",
    "\n",
    "rnn_predict = theano.function([rnn_input_sym, rnn_mask_sym], [rnn_output])\n",
    "print(\"Defined the RNN model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase for the RNN\n",
    "\n",
    "This will take 3-5mins for 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0, iterations_complete = time.time(), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1, iterations_recent = time.time(), iterations_complete\n",
    "epochs=1000*1\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    sentences = batch_sentences()\n",
    "    rnn_input_values, rnn_mask_values, rnn_target_values_int = prep_batch_for_network(sentences, include_targets=True)\n",
    "    \n",
    "    # Now train the RNN\n",
    "    rnn_loss_, = rnn_train(rnn_input_values, rnn_target_values_int, rnn_mask_values)\n",
    "\n",
    "    iterations_complete += 1\n",
    "\n",
    "    if iterations_complete % 10 == 0:\n",
    "        secs_per_batch = float(time.time() - t1)/ (iterations_complete - iterations_recent)\n",
    "        eta_in_secs = secs_per_batch*(epochs-epoch_i)\n",
    "        print(\"Iteration {:5d}, loss_train: {:.4f} ({:.1f}s per 1000 batches)  eta: {:.0f}m{:02.0f}s\".format(\n",
    "                iterations_complete, float(rnn_loss_), \n",
    "                secs_per_batch*1000., np.floor(eta_in_secs/60), np.floor(eta_in_secs % 60), )\n",
    "             )\n",
    "        #print('Iteration {}, output: {}'.format(iteration, disc_output_, ))  # , output: {}\n",
    "        t1, iterations_recent = time.time(), iterations_complete\n",
    "\n",
    "print('Iteration {}, ran in {:.1f}sec'.format(iterations_complete, float(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the learned parameters\n",
    "\n",
    "Uncomment the ```pickle.dump()``` to actually save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_param_values = lasagne.layers.get_all_param_values(rnn_out)\n",
    "rnn_param_dictionary = dict(\n",
    "     params = rnn_param_values,\n",
    "     iterations_complete=iterations_complete,\n",
    "    )\n",
    "pickle.dump(rnn_param_dictionary, open('./data/RNN/tagger_rnn_trained.pkl','wb'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights into network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_param_dictionary = pickle.load(open('./data/RNN/tagger_rnn_trained.pkl', 'rb'))\n",
    "lasagne.layers.set_all_param_values(rnn_out, rnn_param_dictionary['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the Tagger Network 'works'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_results_for(test_sentences):\n",
    "    input_values, mask_values, target_values_int = prep_batch_for_network(test_sentences, include_targets=True)\n",
    "\n",
    "    rnn_output_, = rnn_predict(input_values, mask_values)\n",
    "\n",
    "    # rnn_output_ here is a softmax-vector at every word location\n",
    "    for i,sent in enumerate(test_sentences[0:5]):\n",
    "        annotated = [ \n",
    "                \"%s-%d-%d\" % (word, target_values_int[i,j], np.argmax(rnn_output_[i,j]), )    \n",
    "                for j,word in enumerate(sent) \n",
    "            ]\n",
    "        print(' '.join(annotated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences=[\n",
    "    \"Dr. Andrews works at Red Cat Labs .\",\n",
    "    \"Let 's see what part of speech analysis on this sample text looks like .\",\n",
    "    \"When are you off to New York , Chaitanya ?\",\n",
    "]\n",
    "\n",
    "# Uncomment this for 8 sentences from the corpus\n",
    "#test_sentences = batch_sentences()\n",
    "\n",
    "test_sentences_mixed = [ s.split(' ') for s in sentences ]\n",
    "test_sentences_title = [ s.title().split(' ') for s in sentences ]\n",
    "test_sentences_single = [ s.lower().split(' ') for s in sentences ]\n",
    "#test_sentences_single = [ s.upper().split(' ') for s in sentences ]\n",
    "\n",
    "print(\"Format : WORD-NLTK-RNN\\n\")\n",
    "\n",
    "tag_results_for(test_sentences_mixed)\n",
    "print()\n",
    "tag_results_for(test_sentences_title)\n",
    "print()\n",
    "tag_results_for(test_sentences_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  And let's look at the Statistics\n",
    "\n",
    "... actually, looking at the above samples, the NLTK PoS tagger is HOPELESS when the text is converted to a single case.  QED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1.  Make the tagger identify different PoS (say : 'verbs')\n",
    "\n",
    "2.  Make the tagger return several different tags instead\n",
    "\n",
    "3.  See whether more advanced 'LSTM' nodes would improve the scores\n",
    "\n",
    "4.  Add a special 'is_uppercase' element to the embedding vector (or, more simply, just replace one of the elements with an indicator).  Does this help the NNP accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}