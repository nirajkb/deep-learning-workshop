{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Parallel Texts from TED talks\n",
    "\n",
    "Derived / inspired by : [Ajinkya Kulkarni's GitHub](https://github.com/ajinkyakulkarni14/How-I-Extracted-TED-talks-for-parallel-Corpus-/blob/master/Ipython_notebook.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#import shutil\n",
    "#import codecs\n",
    "import os, glob\n",
    "import csv\n",
    "import time, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enlist_talk_names(url, dict_):\n",
    "    time.sleep( random.random()*5.0+5.0 )\n",
    "    r = requests.get(url)\n",
    "    print(\"  Got %d bytes from %s\" % (len(r.text), url))\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    talks= soup.find_all(\"a\", class_='')\n",
    "    for i in talks:\n",
    "        if i.attrs['href'].find('/talks/')==0 and dict_.get(i.attrs['href'])!=1:\n",
    "            dict_[i.attrs['href']]=1\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_talk_names={}\n",
    "\n",
    "# Get all pages of talks (seems a bit abusive)\n",
    "#for i in xrange(1,61):\n",
    "#    url='https://www.ted.com/talks?page=%d'%(i)\n",
    "#    all_talk_names=enlist_talk_names(url, all_talk_names)\n",
    "\n",
    "# A specific seach term\n",
    "#url='https://www.ted.com/talks?sort=newest&q=ai'\n",
    "\n",
    "# Specific topics\n",
    "url='https://www.ted.com/talks?sort=newest&topics[]=AI'\n",
    "#url='https://www.ted.com/talks?sort=newest&topics[]=machine+learning'\n",
    "#url='https://www.ted.com/talks?sort=newest&topics[]=mind'\n",
    "#url='https://www.ted.com/talks?sort=newest&topics[]=mind&page=2'\n",
    "all_talk_names=enlist_talk_names(url, all_talk_names)\n",
    "len(all_talk_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "def extract_talk_languages(url, talk_name, language_list=['en', 'ko', 'ja']):\n",
    "    need_more_data=False\n",
    "    for lang in language_list:\n",
    "        talk_lang_file = os.path.join(data_path, talk_name+'-'+lang+'.csv')\n",
    "        if not os.path.isfile( talk_lang_file ) :\n",
    "            need_more_data=True\n",
    "    if not need_more_data:\n",
    "        print(\"  Data already retrieved for %s\" % (url,))\n",
    "        return\n",
    "\n",
    "    time.sleep( random.random()*5.0+5.0 )\n",
    "    r = requests.get(url)\n",
    "    print(\"  Got %d bytes from %s\" % (len(r.text), url))\n",
    "    if len(r.text)<1000: return # FAIL!\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for i in soup.findAll('link'):\n",
    "        if i.get('href')!=None and i.attrs['href'].find('?language=')!=-1:\n",
    "            #print i.attrs['href']\n",
    "            lang=i.attrs['hreflang']\n",
    "            url_lang=i.attrs['href']\n",
    "            if not lang in language_list:\n",
    "                continue\n",
    "                \n",
    "            talk_lang_file = os.path.join(data_path, talk_name+'-'+lang+'.csv')\n",
    "            if os.path.isfile( talk_lang_file ) :\n",
    "                continue\n",
    "                \n",
    "            time.sleep( random.random()*5.0+5.0 )\n",
    "            r_lang = requests.get(url_lang)\n",
    "            print(\"    Lang[%s] : Got %d bytes\" % (lang, len(r_lang.text), ))\n",
    "            if len(r.text)<1000: return # FAIL!\n",
    "            lang_soup = BeautifulSoup(r_lang.text, 'html.parser')\n",
    "\n",
    "            talk_data = []\n",
    "            for i in lang_soup.findAll('span',class_='talk-transcript__fragment'):\n",
    "                d = [ int( i.attrs['data-time'] ), i.text.replace('\\n',' ') ]\n",
    "                talk_data.append(d)\n",
    "            \n",
    "            with open(talk_lang_file, 'w') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(['ts', 'txt'])\n",
    "                writer.writerows(talk_data)            \n",
    "\n",
    "if False:\n",
    "    # Now flatten out the talk_data into time_step order\n",
    "    talk_data_csv = [ ['ts']+language_list, ]\n",
    "    for ts in sorted(talk_data.keys(), key=int):\n",
    "        row = [ts] + [ talk_data[ts].get(lang, '') for lang in language_list]\n",
    "        talk_data_csv.append(row)\n",
    "        \n",
    "    with open(os.path.join(data_path, talk_name+'.csv'), 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(talk_data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name in all_talk_names:\n",
    "    extract_talk_languages('https://www.ted.com'+name+'/transcript', name[7:])\n",
    "    #break\n",
    "print(\"Finished extract_talk_languages for all_talk_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}