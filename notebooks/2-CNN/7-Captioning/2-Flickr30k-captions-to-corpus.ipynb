{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flickr30k Captions to Corpus\n",
    "\n",
    "*   P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. _From image description to visual denotations: New similarity metrics for semantic inference over event descriptions._ Transactions of the Association for Computational Linguistics (to appear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "t_start=datetime.datetime.now()\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/Flickr30k'\n",
    "\n",
    "output_dir = './data/cache'\n",
    "\n",
    "output_filepath = os.path.join(output_dir, \n",
    "                                'CAPTIONS_%s_%s.pkl' % ( \n",
    "                                 data_path.replace('./', '').replace('/', '_'),\n",
    "                                 t_start.strftime(\"%Y-%m-%d_%H-%M\"),\n",
    "                                ), )\n",
    "output_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan \n",
    "\n",
    "*  Have a look inside the captions ```flickr30k.tar.gz``` : includes ```results_20130124.token```\n",
    "*  Extract contents of ```flickr30k.tar.gz``` to ```dict( photo_id -> [captions] )```\n",
    "*  Filter out a subset of those ```photo_id``` to convert\n",
    "*  Save off image array and corpus to an easy-to-load filetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_FREQ_MIN=5\n",
    "IMG_WORD_FREQ_MIN=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_captions=dict()\n",
    "\n",
    "tarfilepath = os.path.join(data_path, 'flickr30k.tar.gz')\n",
    "if os.path.isfile(tarfilepath):\n",
    "    import tarfile\n",
    "    with tarfile.open(tarfilepath, 'r:gz').extractfile('results_20130124.token') as tokenized:\n",
    "        n_captions = 0\n",
    "        for l in tokenized.readlines():\n",
    "            #print(l)  # This is bytes\n",
    "            img_num, caption = l.decode(\"utf-8\").strip().split(\"\\t\")\n",
    "            img, num = img_num.split(\"#\")\n",
    "            #print(img, caption); break\n",
    "            if img not in img_to_captions:  img_to_captions[img]=[]\n",
    "            img_to_captions[img].append(caption)\n",
    "            n_captions += 1\n",
    "            \n",
    "print(\"Found %d images, with a total of %d captions\" % (len(img_to_captions),n_captions, ))\n",
    "# Found 31783 images, with a total of 158915 captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_img_to_captions, good_img_to_captions_title = img_to_captions, 'all'\n",
    "len(good_img_to_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter for the images that we care about\n",
    "if False:  \n",
    "    # This is a super-small list, which means we won't get the chance to see \n",
    "    #   enough Text to figure out how to make sentences.  ABANDON THIS 'SIMPLIFICATION'\n",
    "    import re\n",
    "    good_caption = re.compile( r'\\b(cat|kitten)s?\\b', flags=re.IGNORECASE )\n",
    "    good_img_to_captions = { img:captions\n",
    "                                for img, captions in img_to_captions.items() \n",
    "                                for caption in captions \n",
    "                                if good_caption.search( caption )\n",
    "                           }  # img=='3947306345.jpg'\n",
    "    good_img_to_captions_title = 'feline'\n",
    "    #good_img_to_captions\n",
    "    len(good_img_to_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_arr = sorted(good_img_to_captions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the vocab where each word is required to occur WORD_FREQ_MIN times overall\n",
    "word_freq_all=dict()\n",
    "\n",
    "#for img in img_to_captions.keys():  # everything\n",
    "for img in img_arr:  # Our selection\n",
    "    for caption in img_to_captions[img]:\n",
    "        for w in caption.lower().split():\n",
    "            if not w in word_freq_all: word_freq_all[w]=0\n",
    "            word_freq_all[w] += 1\n",
    "            \n",
    "word_freq = { w:f for w,f in word_freq_all.items() if f>=WORD_FREQ_MIN }\n",
    "\n",
    "freq_word = sorted([ (f,w) for w,f in word_freq.items() ], reverse=True)\n",
    "vocab = set( word_freq.keys() )\n",
    "\n",
    "len(vocab), freq_word[0:20]\n",
    "# 7734,  [(271698, 'a'), (151039, '.'), (83466, 'in'), (62978, 'the'), (45669, 'on'), (44263, 'and'), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the vocab where each word is required to occur in IMG_WORD_FREQ_MIN *images* overall\n",
    "word_freq_imgs=dict()\n",
    "\n",
    "#for img in img_to_captions.keys():  # everything\n",
    "for img in img_arr:  # Our selection\n",
    "    img_caption_words=set()\n",
    "    for caption in img_to_captions[img]:\n",
    "        for w in caption.lower().split():\n",
    "            img_caption_words.add(w)\n",
    "    for w in img_caption_words:\n",
    "        if not w in word_freq_imgs: word_freq_imgs[w]=0\n",
    "        word_freq_imgs[w] += 1\n",
    "            \n",
    "word_freq = { w:f for w,f in word_freq_imgs.items() if f>=IMG_WORD_FREQ_MIN }\n",
    "\n",
    "freq_word = sorted([ (f,w) for w,f in word_freq.items() ], reverse=True)\n",
    "vocab = set( word_freq.keys() )\n",
    "\n",
    "len(vocab), freq_word[0:20]\n",
    "# 7219,  [(31783, '.'), (31635, 'a'), (28076, 'in'), (24180, 'the'), (21235, 'is'), (21201, 'and'), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([ (f,w) for w,f in word_freq.items() if not w.isalpha() and '-' not in w ], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set ( stopwords.words('english') )\n",
    "punc = set (\"- . , : ; ' \\\" & $ % ( ) ! ? #\".split())\n",
    "\n",
    "[ (w, w in stop_words) for w in \"while with of at in\".split() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_seen = vocab.intersection( stop_words.union(punc) )\n",
    "\n",
    "', '.join(stop_words_seen)\n",
    "len(stop_words_seen), len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now for the word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = './data/RNN/'\n",
    "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
    "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
    "\n",
    "if not os.path.isfile( glove_100k_50d_path ):\n",
    "    raise RuntimeError(\"You need to download GloVE Embeddings \"+\n",
    "                       \": Use the downloader in 5-Text-Corpus-and-Embeddings.ipynb\")\n",
    "else:\n",
    "    print(\"GloVE available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)\n",
    "import glove\n",
    "embedding_full = glove.Glove.load_stanford( glove_100k_50d_path )\n",
    "embedding_full.word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find words in word_arr that don't appear in GloVe\n",
    "#word_arr = stop_words_seen  # Great : these all have embeddings\n",
    "#word_arr = [ w for w,f in word_freq.items() if f>WORD_FREQ_MIN]  # This seems we're not missing much...\n",
    "word_arr = vocab\n",
    "\n",
    "missing_arr=[]\n",
    "for w in word_arr:\n",
    "    if not w in embedding_full.dictionary:\n",
    "        missing_arr.append(w)\n",
    "len(missing_arr), ', '.join( sorted(missing_arr) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Filter images and vocab jointly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter out the captions for the words that appear in our GloVe embedding\n",
    "#  And ignore the images that then have no captions\n",
    "img_to_valid_captions, words_used = dict(), set()\n",
    "captions_total, captions_valid_total = 0,0\n",
    "\n",
    "for img, captions in good_img_to_captions.items():\n",
    "    captions_total += len(captions)\n",
    "    captions_valid=[]\n",
    "    for caption in captions:\n",
    "        c = caption.lower()\n",
    "        caption_valid=True\n",
    "        for w in c.split():\n",
    "            if w not in embedding_full.dictionary:\n",
    "                caption_valid=False\n",
    "            if w not in vocab:\n",
    "                caption_valid=False\n",
    "        if caption_valid:\n",
    "            captions_valid.append( c )\n",
    "            words_used.update( c.split() )\n",
    "            \n",
    "    if len(captions_valid)>0:\n",
    "        img_to_valid_captions[img]=captions_valid\n",
    "        captions_valid_total += len(captions_valid)\n",
    "    else:\n",
    "        #print(\"Throwing out %s\" % (img,), captions)\n",
    "        pass\n",
    "    \n",
    "print(\"%d images remain of %d.  %d captions remain of %d. Words used : %d\" % (\n",
    "            len(img_to_valid_captions.keys()), len(good_img_to_captions.keys()), \n",
    "            captions_valid_total, captions_total, \n",
    "            len(words_used),)\n",
    "     )\n",
    "# 31640 images remain of 31783.  135115 captions remain of 158915. Words used : 7399 (5 min appearances overall)\n",
    "# 31522 images remain of 31783.  133106 captions remain of 158915. Words used : 6941 (5 min images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So, we only got rid of ~150 images, but 23k captions... if we require 5 mentions minimum\n",
    "# And only got rid of ~250 images, but 25k captions... if we require 5 minimum image appearances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble a ready-for-use embedding\n",
    "\n",
    "Let's filter the embedding to make it sleeker, and add some entries up front for RNN convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct an ordered word list:\n",
    "action_words = \"{MASK} {UNK} {START} {STOP} {EXTRA}\".split(' ')\n",
    "\n",
    "# Then want the 'real words' to have :\n",
    "#  all the stop_words_seen (so that these can be identified separately)\n",
    "#  followed by the remainder of the words_used, in frequency order\n",
    "\n",
    "def words_in_freq_order(word_arr, word_freq=word_freq):\n",
    "    # Create list of freq, word pairs\n",
    "    word_arr_freq = [ (word_freq[w], w) for w in word_arr]\n",
    "    return [ w for f,w in sorted(word_arr_freq, reverse=True) ]\n",
    "\n",
    "stop_words_sorted = words_in_freq_order( stop_words_seen )\n",
    "rarer_words_sorted = words_in_freq_order( words_used - stop_words_seen )\n",
    "\n",
    "#\", \".join( stop_words_sorted )\n",
    "#\", \".join( words_in_freq_order( words_used )[0:100] ) \n",
    "#\", \".join( rarer_words_sorted[0:100] ) \n",
    "len(words_used), len(action_words), len(stop_words_sorted), len(rarer_words_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_full.word_vectors.shape[1]\n",
    "\n",
    "action_embeddings = np.zeros( (len(action_words), EMBEDDING_DIM,), dtype='float32')\n",
    "for idx,w  in enumerate(action_words):\n",
    "    if idx>0:  # Ignore {MASK}\n",
    "        action_embeddings[idx, idx] = 1.0  # Make each row a very simple (but distinct) vector for simplicity\n",
    "\n",
    "stop_words_idx  = [ embedding_full.dictionary[w] for w in stop_words_sorted ]\n",
    "rarer_words_idx = [ embedding_full.dictionary[w] for w in rarer_words_sorted ]\n",
    "\n",
    "embedding = np.vstack([ \n",
    "        action_embeddings,\n",
    "        embedding_full.word_vectors[ stop_words_idx ],\n",
    "        embedding_full.word_vectors[ rarer_words_idx ],\n",
    "    ])\n",
    "\n",
    "embedding_word_arr = action_words + stop_words_sorted + rarer_words_sorted\n",
    "#stop_words_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that this arrangement makes sense :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dictionary = { w:i for i,w in enumerate(embedding_word_arr) }\n",
    "\n",
    "# Check that this all ties together...\n",
    "#word_check='{START}'  # an action word - not found in GloVe\n",
    "#word_check='this'     # a stop word\n",
    "word_check='hammer'   # a 'rare' word\n",
    "\n",
    "#embedding_dictionary[word_check]\n",
    "(  embedding[ embedding_dictionary[word_check] ] [0:6], \n",
    "   embedding_full.word_vectors[ embedding_full.dictionary.get( word_check, 0) ] [0:6], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, save the data into a useful structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)  # Consistent values for train/test (for this )\n",
    "save_me = dict(\n",
    "    img_to_captions = img_to_valid_captions,\n",
    "    \n",
    "    action_words = action_words, \n",
    "    stop_words = stop_words_sorted,\n",
    "    \n",
    "    embedding = embedding,\n",
    "    embedding_word_arr = embedding_word_arr,\n",
    "    \n",
    "    img_arr = img_arr_save,\n",
    "    train_test = np.random.random( (len(img_arr_save),) ),\n",
    ")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "with open( output_filepath, 'wb') as f:\n",
    "    pickle.dump(save_me, f)\n",
    "    \n",
    "print(\"Corpus saved to '%s'\" % (output_filepath,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}