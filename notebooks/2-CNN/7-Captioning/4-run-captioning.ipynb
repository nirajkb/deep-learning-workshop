{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captioning Model(s)\n",
    "\n",
    "This notebook runs the gamut from very basic to very recent models: \n",
    "\n",
    "*  Take the featurized images (2048d), and tokenised captions\n",
    "*  Have 'pluggable' input and output transform for each word :\n",
    "   *   Concat : (256 one-hot - including '0'=mask, '1'={UNK}, '2'={START}, '3'={STOP}, '4'={UseOther})\n",
    "   *   (a) UseOther + (8192-250 of more one-hot)\n",
    "   *   (b) UseOther + (50d of same GloVe embedding, for nearest-neighbour)\n",
    "   *   (c) UseOther + (log2(8192)==13 bits + error correction of index of word ~ 3 copies, averaged)\n",
    "*  LSTM / GRU\n",
    "    *  64d or 200d of hidden units for the RNNs\n",
    "    *  Choice of number of layers\n",
    "    *  Use features as initialisation input for hidden units\n",
    "*  CNN (with dilation \"DeepMind-style\")\n",
    "    *  64d or 200d of width of input, with 1d convolutions run over it\n",
    "    *  Choice of layer layout\n",
    "    *  Use image features as :\n",
    "        * additional channel of input for every timestep with 16x 1x1 convolutions on top; or\n",
    "        * use as additional bias input for word embedding inputs; or\n",
    "        * use an attention-like mechanism to match at each step of the processing\n",
    "*  CNN (including residual layer skips and Gated-Linear-Units \"Facebook-style\")\n",
    "    *  200d of width of input, with 1d convolutions run over it\n",
    "    *  Use image features concatenated onto the word embedding at every step\n",
    "*  Attention-Is-All-You-Need (\"Google-style\")\n",
    "    *  200d of width of input, with new implementation of the AIAYN model in Keras \n",
    "    *  Switchable layouts :\n",
    "        * Can switch off caption self-attention \n",
    "        * Can use more than 1 'layer' deep\n",
    "*  Have a final score for test cases :\n",
    "   *   Just output some captions for a test image or two\n",
    "   *   Calculate BLEU scores (use NLTK : http://www.nltk.org/_modules/nltk/translate/bleu_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "TRAIN_PCT=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in the captions/corpus/embedding\n",
    "with open('./data/cache/CAPTIONS_data_Flickr30k_2017-06-07_23-15.pkl', 'rb') as f:\n",
    "    text_data=pickle.load(f, encoding='iso-8859-1')\n",
    "\n",
    "\"\"\"\n",
    "text_data ~ dict(\n",
    "    img_to_captions = img_to_valid_captions,\n",
    "    \n",
    "    action_words = action_words, \n",
    "    stop_words = stop_words_sorted,\n",
    "    \n",
    "    embedding = embedding,\n",
    "    embedding_word_arr = embedding_word_arr,\n",
    "    \n",
    "    img_arr = img_arr_save,\n",
    "    train_test = np.random.random( (len(img_arr_save),) ),\n",
    ")\"\"\"\n",
    "\n",
    "embedding = text_data['embedding']\n",
    "embedding_eps = 0.00001\n",
    "embedding_normed = embedding / np.maximum( np.linalg.norm(embedding, axis=1, keepdims=True), embedding_eps)\n",
    "vocab_arr = text_data['embedding_word_arr']\n",
    "dictionary = { w:i for i,w in enumerate(vocab_arr) }\n",
    "\n",
    "img_arr_train = [ img for i, img in enumerate(text_data['img_arr']) if text_data['train_test'][i]<TRAIN_PCT ]\n",
    "caption_arr_train = [ (img, caption) for img in img_arr_train for caption in text_data['img_to_captions'][img] ]\n",
    "\n",
    "print(\"Loaded captions, corpus and embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in the features\n",
    "with open('./data/cache/FEATURES_data_Flickr30k_flickr30k-images_2017-06-06_18-07.pkl', 'rb') as f:\n",
    "    image_data=pickle.load(f, encoding='iso-8859-1')\n",
    "\n",
    "\"\"\"\n",
    "image_data ~ dict(\n",
    "    features = features,\n",
    "    img_arr = img_arr,\n",
    ")\n",
    "\"\"\"\n",
    "feature_arr = image_data['features']\n",
    "image_feature_idx = { img:idx for idx, img in enumerate(image_data['img_arr']) }\n",
    "\n",
    "print(\"Loaded dim(%d) image features for %d images\" % (feature_arr.shape[1], feature_arr.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CAPTION_LEN = 32\n",
    "EMBEDDING_DIM = embedding.shape[1]\n",
    "\n",
    "VOCAB_SIZE = len(vocab_arr)\n",
    "LOG2_VOCAB_SIZE = 13  # 1024->10, 8192->13\n",
    "if not (2**LOG2_VOCAB_SIZE/2) < VOCAB_SIZE < 2**LOG2_VOCAB_SIZE:\n",
    "    print(\"LOG2_VOCAB_SIZE incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def caption_to_idx_arr(caption):  # This is actually 1 longer than CAPTION_LEN - need to shift about a bit later\n",
    "    ret = np.zeros( (CAPTION_LEN+1,), dtype='int32')  # {MASK}.idx===0\n",
    "    i=0\n",
    "    ret[i] = dictionary['{START}']\n",
    "    #print(len(caption.split()), caption)\n",
    "    for w in caption.lower().split():\n",
    "        i += 1\n",
    "        ret[i] = dictionary.get(w, dictionary['{UNK}'])\n",
    "    ret[i+1] = dictionary['{STOP}']\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for j in range(0,10):\n",
    "#    print(j)\n",
    "#print(j)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is re-done below, since better to choose over full range of captions, \n",
    "#   rather than randomly within shuffled images\n",
    "def caption_training_example():\n",
    "    img_arr = img_arr_train\n",
    "    while True:\n",
    "        random.shuffle( img_arr )\n",
    "        for img in img_arr:\n",
    "            captions = text_data['img_to_captions'][img]\n",
    "            caption = random.choice(captions)\n",
    "            print(caption)\n",
    "            yield image_feature_idx[ img ], caption_to_idx_arr( caption )\n",
    "        print(\"Captions : Looping\")\n",
    "caption_training_example_gen = caption_training_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "next(caption_training_example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow / Keras imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#    import tensorflow.contrib.keras\n",
    "#    import tensorflow.contrib.keras.backend as K\n",
    "#    from tensorflow.contrib.keras.python.keras.utils.np_utils import to_categorical\n",
    "#    from tensorflow.contrib.keras.api.keras.losses import cosine_proximity, categorical_crossentropy, mean_squared_error\n",
    "#    from tensorflow.contrib.keras.api.keras.activations import softmax, sigmoid\n",
    "#    from tensorflow.contrib.keras.api.keras.layers import Input, Masking, Dense, GRU\n",
    "#    from tensorflow.contrib.keras.api.keras.layers import Activation, Conv1D, Dropout, BatchNormalization\n",
    "#    from tensorflow.contrib.keras.api.keras.layers import RepeatVector, Concatenate, Add, Multiply\n",
    "#    from tensorflow.contrib.keras.api.keras.layers import Permute, Reshape, Dot, Lambda\n",
    "#    from tensorflow.contrib.keras.api.keras.optimizers import RMSprop, Adam\n",
    "#    from tensorflow.contrib.keras.api.keras.models import Model\n",
    "#    from tensorflow.contrib.keras.python.keras.layers import TimeDistributed\n",
    "\n",
    "# This needs keras 2.0.4 for RNN initial_state fixes.  tf.contrib.keras LAGS BADLY as of 2017-06-20\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.losses import cosine_proximity, categorical_crossentropy, mean_squared_error\n",
    "from keras.activations import softmax, sigmoid\n",
    "from keras.layers import Input, Masking, Dense, GRU\n",
    "from keras.layers import Activation, Conv1D, Dropout, BatchNormalization\n",
    "from keras.layers import RepeatVector, Concatenate, Add, Multiply\n",
    "from keras.layers import Permute, Reshape, Dot, Lambda\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create pluggable IO stages for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pick_close_embedding_idx_FIXED_SET(network_output, out_of_top=8):\n",
    "    # cosine distance over whole embedding space\n",
    "    emb_normed = network_output/np.maximum( np.linalg.norm(network_output), embedding_eps)\n",
    "    scores = embedding_normed.dot(emb_normed)\n",
    "    # Find indexes of the top==last few: https://docs.scipy.org/doc/numpy/reference/generated/numpy.argpartition.html\n",
    "    top_idx = np.argpartition(scores, -out_of_top)[-out_of_top:]\n",
    "    top_val = np.maximum( scores[top_idx], embedding_eps)\n",
    "    lo, hi = top_val.min(), top_val.max()\n",
    "    #top_prob = top_val/np.sum(top_val)\n",
    "    top_scaled = (top_val-lo)/(hi-lo)\n",
    "    top_prob = top_scaled/np.sum(top_scaled)\n",
    "    print([ (top_val[i], top_prob[i], vocab_arr[top_idx[i]]) for i in range(out_of_top)])\n",
    "    top_choice = np.random.choice(out_of_top, p=top_prob )\n",
    "    return top_idx[ top_choice ], top_prob[ top_choice ]\n",
    "\n",
    "def pick_close_embedding_idx(network_output, expand_by=1.1, debug=False):\n",
    "    # cosine distance over whole embedding space\n",
    "    emb_normed = network_output/np.maximum( np.linalg.norm(network_output), embedding_eps)\n",
    "    scores = embedding_normed.dot(emb_normed)\n",
    "    \n",
    "    # Find index(es) of the best matches\n",
    "    best_idx = np.argmax(scores)\n",
    "    top_hurdle = 1.0 - (1.0 - scores[best_idx])*expand_by  # Expand the margin away from perfect(==1.0)\n",
    "    \n",
    "    # Now look at all of the scores that are above the hurdle (expand from the best a bit)\n",
    "    top_idx, = np.nonzero(scores > (top_hurdle-embedding_eps)) \n",
    "    n_top = top_idx.shape[0]\n",
    "    top_val = np.maximum( scores[top_idx], embedding_eps)\n",
    "    #lo, hi = top_val.min(), top_val.max()\n",
    "    top_prob = top_val/np.sum(top_val)\n",
    "    #top_scaled = (top_val-lo)/(hi-lo)\n",
    "    #top_prob = top_scaled/np.sum(top_scaled)\n",
    "    if debug: print([ (top_val[i], top_prob[i], vocab_arr[top_idx[i]]) for i in range(n_top)])\n",
    "    top_choice = np.random.choice(n_top, p=top_prob )\n",
    "    return top_idx[ top_choice ], top_prob[ top_choice ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True:  # Test out the embedding decoder \n",
    "    #vec = embedding[ dictionary['to'] ]\n",
    "    #vec = embedding[ dictionary['queen'] ]\n",
    "    vec = embedding[ dictionary['queen'] ] + embedding[ dictionary['king'] ]\n",
    "    for i in range(10):\n",
    "        idx,prob = pick_close_embedding_idx(vec, debug=i==0)\n",
    "        print(\"  %5.2f : %s\" % (prob, vocab_arr[idx],  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pick_softmax_idx(network_output):\n",
    "    e_x = np.exp(network_output - np.max(network_output))\n",
    "    smx = e_x / e_x.sum()  # softmax of the array, better conditioned by the line above\n",
    "    top_idx = np.random.choice(smx.shape[0], p=smx)\n",
    "    return top_idx, smx[top_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class RepresentAs_FullEmbedding():\n",
    "    name = 'FullEmbedding'\n",
    "    width = EMBEDDING_DIM\n",
    "    \n",
    "    def encode(caption_arr):\n",
    "        # plain embedding of each symbol\n",
    "        return embedding[ caption_arr, : ]\n",
    "\n",
    "    def loss_fn(ideal_output, network_output):  # y_true, y_pred\n",
    "        return cosine_proximity( ideal_output, network_output )\n",
    "    \n",
    "    def decode(network_output):\n",
    "        #return '{UNK}', 1.0  # placeholder (word and confidence level)\n",
    "        idx, prob = pick_close_embedding_idx(network_output)\n",
    "        return vocab_arr[ idx ], prob\n",
    "    \n",
    "class RepresentAs_FullOneHot():\n",
    "    name = 'FullOneHot'\n",
    "    width = VOCAB_SIZE\n",
    "    \n",
    "    def encode(caption_arr):\n",
    "        # Output desired is one-hot of each symbol (cross entropy match whole thing) \n",
    "        return to_categorical(caption_arr, num_classes=VOCAB_SIZE) \n",
    "    \n",
    "    def loss_fn(ideal_output, network_output):  # y_true, y_pred\n",
    "        smx = softmax(network_output, axis=-1)\n",
    "        return categorical_crossentropy( ideal_output, smx )\n",
    "    \n",
    "    def decode(network_output):\n",
    "        idx, prob = pick_softmax_idx(network_output)\n",
    "        return vocab_arr[ idx ], prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_width = len(text_data['action_words']) + len(text_data['stop_words'])\n",
    "MASK_idx = dictionary['{MASK}']   # ==0\n",
    "EXTRA_idx = dictionary['{EXTRA}']\n",
    "UNK_idx = dictionary['{UNK}']\n",
    "\n",
    "def OneHotBasePlus(arr): # Contains indexing magic\n",
    "    #  Arrangement should be :: (samples, timesteps, features),\n",
    "    one_hot_base_plus = np.zeros( (CAPTION_LEN, base_width), dtype='float32')\n",
    "    # Set the indicator for entries not in action or stop words\n",
    "    one_hot_base_plus[ arr>=base_width, EXTRA_idx ] = 1.0\n",
    "    # Set the one-hot for everthing in the one-hot-region\n",
    "    one_hot_base_plus[ arr< base_width, arr[np.where(arr<base_width)] ] = 1.0\n",
    "    # Force masked values to all-zeros\n",
    "    one_hot_base_plus[ arr==0, MASK_idx ] = 0.0\n",
    "    return one_hot_base_plus\n",
    "\n",
    "class RepresentAs_OneHotBasePlusEmbedding():\n",
    "    name = 'OneHotBasePlusEmbedding'\n",
    "    width = base_width + EMBEDDING_DIM\n",
    "\n",
    "    def encode(caption_arr): \n",
    "        # Input is onehot for first part, with the embedding included for all words too\n",
    "        return np.hstack( [ OneHotBasePlus(caption_arr), embedding[caption_arr] ] )\n",
    "    \n",
    "    def loss_fn(ideal_output, network_output):  # y_true, y_pred\n",
    "        # One-hot of each action symbol and stop words (cross entropy match these) and \n",
    "        #   RMSE on remaining embedding (weighted according to onehot[{EXTRA}]~0...1)\n",
    "        \n",
    "        #print(\"ideal_output.shape\", ideal_output.shape)     # ideal_output.shape (?, ?, ?)\n",
    "        #print(\"network_output.shape\", network_output.shape) # network_output.shape (?, 32, 191)\n",
    "        \n",
    "        # Perhaps need this idea https://github.com/fchollet/keras/issues/890:\n",
    "        smx = softmax(network_output[:, :, :base_width], axis=-1)\n",
    "        #print(\"smx.shape\", smx.shape) # smx.shape (?, 32, 141)\n",
    "        \n",
    "        is_extra = smx[:, :, EXTRA_idx]\n",
    "        one_hot_loss = categorical_crossentropy( ideal_output[:, :, :base_width], smx )    \n",
    "        embedding_loss = cosine_proximity( ideal_output[:, :, base_width:], \n",
    "                                                network_output[:, :, base_width:] )\n",
    "        \n",
    "        return (1.-is_extra)*one_hot_loss + (is_extra)*embedding_loss\n",
    "    \n",
    "    def decode(network_output):\n",
    "        idx, prob = pick_softmax_idx(network_output[:base_width])\n",
    "        if idx==EXTRA_idx:\n",
    "            idx, prob2 = pick_close_embedding_idx(network_output[base_width:])\n",
    "            prob *= prob2\n",
    "        return vocab_arr[ idx ], prob\n",
    "    \n",
    "    \n",
    "POWERS_OF_2 = 2**np.arange(LOG2_VOCAB_SIZE)\n",
    "class RepresentAs_OneHotBasePlusBinaryIdx():\n",
    "    name = 'OneHotBasePlusBinaryIdx'\n",
    "    width = base_width + 3*LOG2_VOCAB_SIZE\n",
    "\n",
    "    def encode(caption_arr):\n",
    "        # Input is onehot for first part, with 3 copies of the binary index of all words afterwards\n",
    "        #   Idea is from : https://arxiv.org/abs/1704.06918\n",
    "\n",
    "        # Thanks to : https://stackoverflow.com/questions/21918267/\n",
    "        #         convert-decimal-range-to-numpy-array-with-each-bit-being-an-array-element\n",
    "        binary = (caption_arr[:, np.newaxis] & POWERS_OF_2) / POWERS_OF_2\n",
    "        binary -= 0.5  # symmetrical around 0\n",
    "        \n",
    "        return np.hstack( [ OneHotBasePlus(caption_arr), binary, binary, binary ] )\n",
    "  \n",
    "    def loss_fn(ideal_output, network_output):  # y_true, y_pred\n",
    "        smx = softmax(network_output[:base_width], axis=-1)\n",
    "        sig = sigmoid(network_output[base_width:])\n",
    "        \n",
    "        is_extra = smx[:, :, EXTRA_idx]\n",
    "        one_hot_loss = categorical_crossentropy( ideal_output[:base_width], smx )\n",
    "        #binary_loss  = categorical_crossentropy( ideal_output[base_width:], sig )\n",
    "        binary_loss  = mean_squared_error( ideal_output[base_width:], sig )  # reported better in paper\n",
    "        return (1.-is_extra)*one_hot_loss + (is_extra)*binary_loss\n",
    "    \n",
    "    def decode(network_output, debug=False):\n",
    "        idx, prob = pick_softmax_idx(network_output[:base_width])\n",
    "        if idx==EXTRA_idx:\n",
    "            sig = 1./(1. + np.exp(-network_output[base_width:] ))\n",
    "            #print(sig)\n",
    "            binary = np.mean( sig.reshape( (3, LOG2_VOCAB_SIZE) ), axis=0 )>0.5\n",
    "            if debug: \n",
    "                print( sig )\n",
    "                #print( sig.shape )\n",
    "                print( sig.reshape( (3, LOG2_VOCAB_SIZE) ) )\n",
    "            # TODO : more work to do something (a) stochastic, and (b) give 'prob' measure\n",
    "            idx = POWERS_OF_2.dot(binary>0.5)\n",
    "            if idx>VOCAB_SIZE: idx = UNK_idx\n",
    "        return vocab_arr[ idx ], prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just a little test area for the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#io = RepresentAs_FullEmbedding             # .encode : 32, 50\n",
    "#io = RepresentAs_FullOneHot                # .encode : 32, 6946\n",
    "io = RepresentAs_OneHotBasePlusEmbedding   # .encode : 32, 191 \n",
    "#io = RepresentAs_OneHotBasePlusBinaryIdx   # .encode : 32, 180\n",
    "io.width\n",
    "\n",
    "caption_sample = 'The cat sat on the mat .'\n",
    "caption_sample_idx = caption_to_idx_arr(caption_sample)\n",
    "caption_sample_idx  # array([   2,    8, 1461, 2496,   11,    8,  998,    5,    3,    0,    0...\n",
    "\n",
    "onehot_start=range(0,12)\n",
    "x=OneHotBasePlus(caption_sample_idx[:-1])  # Just the first 10 one-hot entries\n",
    "#x\n",
    "#x[onehot_start, dictionary['{MASK}']]\n",
    "#x[onehot_start, dictionary['{START}']]\n",
    "#x[onehot_start,  EXTRA_idx]\n",
    "#x[onehot_start, dictionary['{STOP}']]\n",
    "#x[onehot_start, dictionary['on']]\n",
    "\n",
    "#x.shape  # 32, 141 \n",
    "#embedding[caption_sample_idx[:-1]].shape  # 32, 50\n",
    "\n",
    "if False:\n",
    "    powers_of_two = 2**np.arange(LOG2_VOCAB_SIZE)\n",
    "    (caption_sample_idx[:, np.newaxis] & powers_of_two) / powers_of_two\n",
    "\n",
    "io.encode( caption_sample_idx[:-1] ).shape  # [0:6,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# uses io as defined above\n",
    "per_caption = CAPTION_LEN//2\n",
    "for i in range(0, len(vocab_arr), per_caption):\n",
    "    caption_sample = ' '.join( vocab_arr[i : i+per_caption ] )\n",
    "    caption_sample_idx = caption_to_idx_arr(caption_sample)\n",
    "    print(\"%4d : %s \" % (i,caption_sample,))\n",
    "    x=io.encode(caption_sample_idx[:-1])  # This is now an embedding\n",
    "    #print(x.shape)\n",
    "    #print( x[1, -LOG2_VOCAB_SIZE:]+0.5)\n",
    "    x *= 12.0  # By virtue of how this works... won't affect embedding, will make softmax more precise\n",
    "    caption_restore = ' '.join([ io.decode(x[i+1])[0] for i in range(per_caption)])  # , debug=i==0\n",
    "    print(\"       %s \" % (caption_restore,))\n",
    "    if i>250: break       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create Batches for given embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#BATCH_SIZE=16    # Titan-X occupancy ~ 37%, epoch ~ 450sec for GRUs\n",
    "BATCH_SIZE=64    # Titan-X occupancy ~ 32%, epoch ~ 300sec for GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def caption_example_generator():\n",
    "    while True:\n",
    "        random.shuffle( caption_arr_train ) \n",
    "        for img, caption in caption_arr_train:\n",
    "            if len(caption.split())>CAPTION_LEN-2 : continue # Skip captions that are too long\n",
    "            yield img, caption\n",
    "\n",
    "def caption_training_batch_generator(emb_input, emb_output, batch_size=BATCH_SIZE):\n",
    "    caption_example_gen = caption_example_generator()\n",
    "    while True:\n",
    "        X0, X1, Y = [],[], []\n",
    "        for _ in range(batch_size):\n",
    "            img, caption = next( caption_example_gen )\n",
    "            caption_idx = caption_to_idx_arr(caption)\n",
    "            X0.append( feature_arr[ image_feature_idx[img] ]  )\n",
    "            X1.append( emb_input.encode(caption_idx[:-1]) )\n",
    "            Y .append( emb_output.encode(caption_idx[1:]) )\n",
    "        yield [np.array(X0), np.array(X1)], [np.array(Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "caption_example_gen = caption_example_generator()\n",
    "next(caption_example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Functions that return Models\n",
    "Here, the ```caption``` input is the result of one of the embedding classes above (batched up), so has some useful properties.  The ```features``` input is just the features created by the InceptionV3 featurisation already loaded (batched up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plain xxx2seq model\n",
    "This is a 'plain' xxx2seq kind of model, where the feature vector for X is fed in as the initial hidden state of the RNN (which is composed of GRUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def RNN_captioner(rnn_count, \n",
    "                  caption_input_shape=None, feature_shape=None, caption_output_shape=None, \n",
    "                  levels=1):  \n",
    "    feature_in = Input(shape=feature_shape, dtype='float32', name='feature_input')\n",
    "    feature_downsize = Dense(rnn_count)(feature_in) \n",
    "    \n",
    "    caption_in = Input(shape=caption_input_shape, dtype='float32', name='caption_input')\n",
    "    #masked = Masking(mask_value=0.)(caption_in)  # , input_shape=caption_input_shape\n",
    "    masked = caption_in # Ignore the masking thing - this will be implicit in the scoring\n",
    "\n",
    "    #  initial_state = feature_downsize :: \n",
    "    # See : https://github.com/fchollet/keras/issues/2995\n",
    "    #       https://github.com/fchollet/keras/pull/3947 (closed, unmerged)\n",
    "    # Hidden in the code : https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py#L178\n",
    "    \n",
    "    rnn = masked\n",
    "    \n",
    "    # The recurrent layer\n",
    "    #rec1 = GRU(rnn_count, initial_state=feature_downsize, return_sequences=True)(rnn)\n",
    "    rnn = GRU(rnn_count, return_sequences=True)(rnn, initial_state=feature_downsize)\n",
    "    #rec1 = GRU(rnn_count, return_sequences=True)(rnn)\n",
    "\n",
    "    for _ in range(levels-1):\n",
    "        rnn = GRU(rnn_count, return_sequences=True)(rnn)\n",
    "    \n",
    "    rnn_outputs = rnn\n",
    "\n",
    "    #print( (BATCH_SIZE, caption_input_shape[0], rnn_count) )\n",
    "    caption_out = TimeDistributed( Dense(caption_output_shape[1], activation='linear'), # activation='softmax'\n",
    "                                   input_shape=(-1, caption_input_shape[0], rnn_count),\n",
    "                                   name='output-sizing')(rnn_outputs)\n",
    "    \n",
    "    return Model(inputs=[feature_in, caption_in], \n",
    "                 outputs=[caption_out], \n",
    "                 name='RNN-captioner-nomask-%dx%d' % (levels, rnn_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN with Dilations Model (\"DeepMind\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def CNN_captioner(cnn_channels, \n",
    "                  caption_input_shape=None, feature_shape=None, caption_output_shape=None, \n",
    "                  layout=0):\n",
    "    feature_in = Input(shape=feature_shape, dtype='float32', name='feature_input')\n",
    "    \n",
    "    feature_resize = Dense(cnn_channels, name='FeatureProjection')(feature_in) \n",
    "    feature_everywhere = RepeatVector(caption_input_shape[0], name='RepeatedFeatures')(feature_resize)\n",
    "    \n",
    "    caption_in = Input(shape=caption_input_shape, dtype='float32', name='caption_input')\n",
    "\n",
    "    if layout==0 or layout==1:\n",
    "        caption_resize = Dense(cnn_channels)(caption_in) ## Is this right?  time-dependency?\n",
    "        \n",
    "        #caption_features = Concatenate( [caption_resize, feature_resize], axis=3) # several channels\n",
    "\n",
    "        # So the image feature becomes a bias for all the first layer cnn inputs\n",
    "        caption_features = Add()( [feature_everywhere, caption_resize] )\n",
    "        cnn = Activation('relu')( caption_features )\n",
    "        \n",
    "    if layout==2 or layout==3 or layout==4 or layout==5:\n",
    "        caption_cnn = Conv1D(2*cnn_channels, 1, padding='causal', dilation_rate=1, activation='relu')(caption_in)\n",
    "\n",
    "        # So the image features can be added into the embedding as required\n",
    "        caption_features = Concatenate()( [feature_everywhere, caption_cnn] )\n",
    "        cnn = ( caption_features )  # Don't do any relu transform at this point\n",
    "\n",
    "    if layout==6:\n",
    "        feature_everywhere_full = RepeatVector(caption_input_shape[0], name='RepeatedFeatures')(feature_in)\n",
    "        \n",
    "        caption_cnn = Conv1D(2*cnn_channels, 1, padding='causal', dilation_rate=1, activation='relu')(caption_in)\n",
    "\n",
    "        # So the image features can be added into the embedding as required\n",
    "        caption_features = Concatenate()( [feature_everywhere_full, caption_cnn] )\n",
    "        cnn = ( caption_features )  # Don't do any relu transform at this point\n",
    "        \n",
    "    if layout==0 or layout==1 or layout==2 or layout==3 or layout==4 or layout==5 or layout==6:\n",
    "        # The CNN layers\n",
    "        # Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, \n",
    "        #        activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "\n",
    "        cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=1, activation='relu')(cnn)\n",
    "\n",
    "        if layout==3 or layout==4:\n",
    "            # Gated Linear Units\n",
    "            cnn_gate = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=1, activation='tanh')(cnn)\n",
    "            cnn = Multiply()( [cnn, cnn_gate] )\n",
    "        \n",
    "        if layout==1 or layout==4:\n",
    "            cnn = Dropout(0.5)(cnn)\n",
    "            \n",
    "        if layout==4 or layout==5:\n",
    "            cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=1, activation='relu')(cnn)\n",
    "            cnn = BatchNormalization(scale=False)(cnn)\n",
    "            \n",
    "        cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=2, activation='relu')(cnn)\n",
    "        \n",
    "        if layout==1 or layout==4:\n",
    "            cnn = Dropout(0.5)(cnn)\n",
    "            \n",
    "        cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=4, activation='relu')(cnn)\n",
    "        \n",
    "        if layout==1 or layout==4:\n",
    "            cnn = Dropout(0.5)(cnn)\n",
    "            \n",
    "        cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=8, activation='relu')(cnn)\n",
    "        cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=16, activation='relu')(cnn)\n",
    "        cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=1, activation='relu')(cnn)\n",
    "        \n",
    "        if layout==4 or layout==5:\n",
    "            cnn = BatchNormalization(scale=False)(cnn)\n",
    "        \n",
    "        #if layout==5:\n",
    "        #    # Gated Linear Units\n",
    "        #    cnn_gate = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=1, activation='tanh')(cnn)\n",
    "        #    cnn = Multiply()( [cnn, cnn_gate] )\n",
    "        \n",
    "        if layout==3 or layout==4:\n",
    "            cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=2, activation='relu')(cnn)\n",
    "            cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=1, activation='relu')(cnn)\n",
    "\n",
    "        if layout==5:            \n",
    "            cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=2, activation='relu')(cnn)\n",
    "            cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=4, activation='relu')(cnn)\n",
    "            cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=8, activation='relu')(cnn)\n",
    "            cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=16, activation='relu')(cnn)\n",
    "            cnn = Conv1D(cnn_channels, 3, padding='causal', dilation_rate=1, activation='relu')(cnn)\n",
    "            \n",
    "        cnn_outputs = cnn\n",
    "\n",
    "    caption_out = TimeDistributed( Dense(caption_output_shape[1], activation='linear'), # activation='softmax'\n",
    "                                   input_shape=(-1, caption_input_shape[0], cnn_channels),\n",
    "                                   name='output-sizing')(cnn_outputs)\n",
    "    \n",
    "    return Model(inputs=[feature_in, caption_in], \n",
    "                 outputs=[caption_out], \n",
    "                 name='CNN-captioner-%dx%d' % (layout, cnn_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN with Gated Linear Units (\"Facebook\")\n",
    "\n",
    "The \"Convolutional Sequence to Sequence Learning\" paper https://arxiv.org/abs/1705.03122 was released 6 weeks ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def CNNGLU_layer(cnn_channels, k, dilation_rate=1, residual=True, batch_norm=True):\n",
    "    # for original paper, see : https://arxiv.org/abs/1612.08083\n",
    "    def layer(cnn):\n",
    "        cnnA = Conv1D(cnn_channels, k, padding='causal', dilation_rate=dilation_rate, activation='linear')(cnn)\n",
    "        cnnB = Conv1D(cnn_channels, k, padding='causal', dilation_rate=dilation_rate, activation='sigmoid')(cnn)\n",
    "        cnnOut = Multiply()( [cnnA, cnnB] )\n",
    "        if residual:\n",
    "            cnnOut = Add()([cnn, cnnOut])  # This is the residual skip\n",
    "        if batch_norm:\n",
    "            cnnOut = BatchNormalization()(cnnOut)\n",
    "        return cnnOut\n",
    "    return layer\n",
    "\n",
    "def CNNGLU_captioner(cnn_channels, \n",
    "                     caption_input_shape=None, feature_shape=None, caption_output_shape=None, \n",
    "                     layout=0):\n",
    "    feature_in = Input(shape=feature_shape, dtype='float32', name='feature_input')\n",
    "    feature_everywhere_full = RepeatVector(caption_input_shape[0], name='RepeatedFeatures')(feature_in)\n",
    "    \n",
    "    caption_in = Input(shape=caption_input_shape, dtype='float32', name='caption_input')\n",
    "    caption_cnn = Conv1D(4*cnn_channels, 1, padding='causal', dilation_rate=1, activation='relu')(caption_in)\n",
    "\n",
    "    # So the image features can be added into the embedding as required\n",
    "    caption_features = Concatenate()( [feature_everywhere_full, caption_cnn] )\n",
    "    cnn = Conv1D(cnn_channels, 1, padding='causal', dilation_rate=1, activation='relu')( caption_features )\n",
    "\n",
    "    if layout==7:\n",
    "        cnn = CNNGLU_layer(cnn_channels, 3, dilation_rate=1)(cnn)\n",
    "        cnn = CNNGLU_layer(cnn_channels, 3, dilation_rate=2)(cnn)\n",
    "        cnn = CNNGLU_layer(cnn_channels, 3, dilation_rate=4)(cnn)\n",
    "        cnn = CNNGLU_layer(cnn_channels, 3, dilation_rate=8)(cnn)\n",
    "        cnn = CNNGLU_layer(cnn_channels, 3, dilation_rate=16)(cnn)\n",
    "\n",
    "        cnn_outputs = cnn\n",
    "\n",
    "    caption_out = TimeDistributed( Dense(caption_output_shape[1], activation='linear'), # activation='softmax'\n",
    "                                   input_shape=(-1, caption_input_shape[0], cnn_channels),\n",
    "                                   name='output-sizing')(cnn_outputs)\n",
    "    \n",
    "    return Model(inputs=[feature_in, caption_in], \n",
    "                 outputs=[caption_out], \n",
    "                 name='CNNGLU-captioner-%dx%d' % (layout, cnn_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \"Attention is All You Need\" (\"Google\")\n",
    "\n",
    "The \"Attention is All You Need\" paper : https://arxiv.org/abs/1706.03762 was released 1 week before this was originally presented.  So, as far as I know, it was the first Keras version of this innovative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ResidualAndNormLayer(name):\n",
    "    def layer(before, after):\n",
    "        out = Add(name=name+'res')([before, after])      # This is the residual step\n",
    "        return BatchNormalization(name=name+'bn')(out)\n",
    "    return layer\n",
    "\n",
    "def AttentionLayer(name, n_heads=4, key_widths=32, value_width=50, mask=None):\n",
    "    def layer(q_var, k_var, v_var):\n",
    "        sqrt_dk = np.sqrt(float(key_widths))\n",
    "        v_contrib = []\n",
    "        for i in range(n_heads):\n",
    "            n=\"%s%i\" % (name, i)\n",
    "            #print(i, key_widths, value_width, q_var.shape)\n",
    "            q_head = TimeDistributed( Dense(key_widths), name=n+\"Q\")(q_var) # shape ~ (?, time_step, key_width)\n",
    "            k_head = TimeDistributed( Dense(key_widths), name=n+\"K\")(k_var) # shape ~ (?, time_step, key_width)\n",
    "\n",
    "            #   We need to do this across time, so the dot products pile up each location\n",
    "            k_head_transpose = Permute( (2, 1), name=n+\"K.T\")(k_head)  # shape ~ (?, key_width, time_step)\n",
    "\n",
    "            # Give each position an array with corresponds to amount it 'wants' each other location\n",
    "            fit_head = Dot([2, 1], normalize=False, name=n+\"match\")( [q_head, k_head_transpose] )\n",
    "\n",
    "            # Scale this down to compensate for adding together lots of N()*N()\n",
    "            fit_head_scaled = Lambda(lambda x: x/sqrt_dk, name=n+\"scaled\")(fit_head)\n",
    "\n",
    "            fit_head_masked = fit_head_scaled\n",
    "            if mask is not None:\n",
    "                # Mask out backwards (in time) flowing information (this is the decoder), by adding '-100.'\n",
    "                #fit_head_masked = K.clip(fit_head_masked, -20., +20.)  # set bounds so -100. is ~ -inf\n",
    "                fit_head_masked = Lambda(lambda x: K.clip(x, -20., +20.))(fit_head_masked)  # set bounds so -100. is ~ -inf\n",
    "                fit_head_masked = Add()( [fit_head_masked, mask] )\n",
    "\n",
    "            # Give each position an array with corresponds to 'probability' it 'wants' each other location\n",
    "            max_head = TimeDistributed( Activation(K.softmax), name=n+\"SoftMax\" ) ( fit_head_masked )  \n",
    "\n",
    "            # Now the value side\n",
    "            v_head = TimeDistributed( Dense(value_width), name=n+\"V\" )(v_var) \n",
    "\n",
    "            # Combine the key/query match softmax output with the values\n",
    "            value_head = Dot([2, 1], normalize=False, name=n+\"Vcontrib\")( [max_head, v_head] )\n",
    "\n",
    "            v_contrib.append(value_head)\n",
    "\n",
    "        if n_heads>1:\n",
    "            v_return = Concatenate(name=n+\"Concat\")( v_contrib )\n",
    "        else:\n",
    "            v_return = v_contrib[0]\n",
    "        return v_return\n",
    "    return layer\n",
    "\n",
    "\n",
    "def AIAYN_captioner(internal_width, \n",
    "                    caption_input_shape=None, feature_shape=None, caption_output_shape=None,\n",
    "                    layout=0):\n",
    "    n_time_steps = caption_input_shape[0]\n",
    "    \n",
    "    feature_in = Input(shape=feature_shape, dtype='float32', name='feature_input')\n",
    "    features_everywhere = RepeatVector(n_time_steps, name='RepeatedFeatures')(feature_in)\n",
    "    \n",
    "    caption_in = Input(shape=caption_input_shape, dtype='float32', name='caption_input')\n",
    "\n",
    "    # Make the caption embedding fit the 'internal_width' \n",
    "    x = Dense(internal_width, name='InitialDense')(caption_in)\n",
    "    \n",
    "    def BatchNumpyConst(np_var):  # (needs a 'bogus input' for batch_sizing...)\n",
    "        # Need to preserve batch-sizes\n",
    "        def output_of_lambda(input_shape):\n",
    "            return (input_shape[0], np_var.shape[0], np_var.shape[1])\n",
    "        \n",
    "        #  This is 1x(what we want), but the 'output_shape' causes broadcasting\n",
    "        return Lambda(lambda x: K.constant(np_var[None]), output_shape=output_of_lambda)( caption_in )\n",
    "    \n",
    "    # Add the 'clocks'\n",
    "    def ClockVar(width):\n",
    "        clock_theta  = np.arange( 0, 1.0, 2.0/n_time_steps) * np.pi\n",
    "        clock_offset = (2. + np.arange( 0, width ))\n",
    "        clock_half   = np.outer( clock_theta, clock_offset ) \n",
    "        clock_const  = np.vstack( [ np.cos(clock_half), np.sin(clock_half) ] ).astype('float32')\n",
    "        return BatchNumpyConst(clock_const)\n",
    "\n",
    "    features = features_everywhere\n",
    "    if True:\n",
    "        features_with_clocks = Add(name='FeatWithClocks')( [features_everywhere, ClockVar(feature_shape[0])] )\n",
    "        features = features_with_clocks\n",
    "    \n",
    "    x = Add(name='WithClocks')( [x, ClockVar(internal_width) ] )\n",
    "    \n",
    "    # create a mask to prevent backwards-in-time message passing during training\n",
    "    mask_timewise = np.tri(n_time_steps, n_time_steps, -1).T * -100.0  # ~ -infinity\n",
    "    mask_const    = BatchNumpyConst(mask_timewise)\n",
    "    \n",
    "    n_heads = 1\n",
    "    if layout==3 or layout==4: n_heads=4\n",
    "    params = dict( n_heads=n_heads, key_widths=32, value_width=internal_width//n_heads )\n",
    "    \n",
    "    n_layers = 1\n",
    "    if layout==4: n_layers=2\n",
    "    for i in range(n_layers):\n",
    "        if layout==1:  # Before the 'image features' - as in the paper\n",
    "            # Attention layer looking over the previous words in caption\n",
    "            x_attend = AttentionLayer('L%d-C-'%i, mask=mask_const, **params)(x, x, x)\n",
    "            x = ResidualAndNormLayer('L%d-CN-'%i)(x, x_attend)\n",
    "            \n",
    "        if True:\n",
    "            # Attention layer looking over the features of the image\n",
    "            x_attend = AttentionLayer('L%d-I-'%i, mask=None, **params)(x, features, features)\n",
    "            x = ResidualAndNormLayer('L%d-IN-'%i)(x, x_attend)\n",
    "\n",
    "        if layout==2 or layout==3 or layout==4: # After seeing some features (more variety for first word)\n",
    "            # Attention layer looking over the previous words in caption\n",
    "            x_attend = AttentionLayer('L%d-C-'%i, mask=mask_const, **params)(x, x, x)\n",
    "            x = ResidualAndNormLayer('L%d-CN-'%i)(x, x_attend)\n",
    "\n",
    "        # Dense Feed-Forward Network \"FFN\"\n",
    "        x_ff = Dense(2*internal_width, name='FF%d-1'%i, activation='relu')(x)\n",
    "        x_ff = Dense(  internal_width, name='FF%d-2'%i, activation='linear')(x_ff)\n",
    "        x = ResidualAndNormLayer('FF%d-N'%i)(x, x_ff)\n",
    "\n",
    "    caption_out = TimeDistributed( Dense(caption_output_shape[1], activation='linear'), # activation='softmax'\n",
    "                                   input_shape=(-1, caption_input_shape[0], internal_width),\n",
    "                                   name='output-sizing')(x)\n",
    "    \n",
    "    return Model(inputs=[feature_in, caption_in], \n",
    "                 outputs=[caption_out], \n",
    "                 name='AIAYN-captioner-%dx%d' % (layout, internal_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the embedding to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emb_input, emb_output = RepresentAs_OneHotBasePlusEmbedding, RepresentAs_FullOneHot\n",
    "#emb_input, emb_output = RepresentAs_OneHotBasePlusEmbedding, RepresentAs_OneHotBasePlusEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And chose the specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_choice = \"xRNN CNN xCNNGLU xAIAYN\".split()\n",
    "\n",
    "if 'RNN' in model_choice:\n",
    "    model = RNN_captioner(200, \n",
    "                      caption_input_shape=(CAPTION_LEN, emb_input.width), \n",
    "                      feature_shape=(feature_arr.shape[1],),\n",
    "                      caption_output_shape=(CAPTION_LEN, emb_output.width),\n",
    "                      levels=2,\n",
    "                     )\n",
    "\n",
    "if 'CNN' in model_choice:\n",
    "    model = CNN_captioner(200, \n",
    "                      caption_input_shape=(CAPTION_LEN, emb_input.width), \n",
    "                      feature_shape=(feature_arr.shape[1],),\n",
    "                      caption_output_shape=(CAPTION_LEN, emb_output.width),\n",
    "                      layout=5,\n",
    "                     )\n",
    "\n",
    "if 'CNNGLU' in model_choice:\n",
    "    model = CNNGLU_captioner(200, \n",
    "                      caption_input_shape=(CAPTION_LEN, emb_input.width), \n",
    "                      feature_shape=(feature_arr.shape[1],),\n",
    "                      caption_output_shape=(CAPTION_LEN, emb_output.width),\n",
    "                      layout=7,\n",
    "                     )\n",
    "\n",
    "if 'AIAYN' in model_choice:\n",
    "    model = AIAYN_captioner(200, \n",
    "                      caption_input_shape=(CAPTION_LEN, emb_input.width), \n",
    "                      feature_shape=(feature_arr.shape[1],),\n",
    "                      caption_output_shape=(CAPTION_LEN, emb_output.width),\n",
    "                      layout=4,\n",
    "                     )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.compile(loss=emb_output.loss_fn, optimizer=RMSprop(lr=0.0001, clipnorm=1.))\n",
    "model.compile(loss=emb_output.loss_fn, optimizer=Adam())\n",
    "\n",
    "# Idea : Change learning rates via callbacks : https://github.com/fchollet/keras/issues/2823"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for testing a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sample_caption(model, features):\n",
    "    # Run the model step-by-step, performing a decode, and re-inputting the result into the inputs...\n",
    "    caption_arr, prob_tot=[], 0  # initially empty\n",
    "    for i in range(CAPTION_LEN):\n",
    "        caption_idx = caption_to_idx_arr(' '.join(caption_arr))\n",
    "        caption_emb_in = emb_input.encode(caption_idx[:-1])\n",
    "        \n",
    "        # Need to make features and caption_emb_in into length-1 batches\n",
    "        caption_emb_out = model.predict_on_batch( [ features[np.newaxis], caption_emb_in[np.newaxis] ] )\n",
    "        \n",
    "        # pick out the i-th output, and add it onto the \n",
    "        caption_word, caption_prob = emb_output.decode(caption_emb_out[0][i])\n",
    "        \n",
    "        #print(\"%.2f %4d %s\" % (caption_prob, dictionary[caption_word], caption_word,))\n",
    "        \n",
    "        if caption_word=='{STOP}': break\n",
    "        caption_arr.append(caption_word)\n",
    "        prob_tot += caption_prob  # I know this isn't correct - just a temp value\n",
    "        \n",
    "    return ' '.join(caption_arr), prob_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def model_test(model, img_idx, img_path='./data/Flickr30k/flickr30k-images/'):\n",
    "    # first, let's just show the image, and the existing captions\n",
    "    \n",
    "    img_name = text_data['img_arr'][img_idx]\n",
    "    if True:\n",
    "        is_training = text_data['train_test'][img_idx]\n",
    "        captions = text_data['img_to_captions'][img_name]\n",
    "\n",
    "        print(\"Image is in %s set\" % ('TRAINING' if is_training<TRAIN_PCT else 'TEST',) )\n",
    "        for caption in captions:\n",
    "            print(\"  * %s\" % (caption, ))\n",
    "    \n",
    "    if True:\n",
    "        img_filepath = os.path.join(img_path, img_name)\n",
    "        img_data = plt.imread(img_filepath)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(img_data)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    features = feature_arr[ image_feature_idx[img_name] ] \n",
    "    for _ in range(5):\n",
    "        caption, prob = sample_caption(model, features)\n",
    "        print(\"  * %s\" % (caption,))\n",
    "\n",
    "# Find some TEST images\n",
    "#print( [ \"%d:%.2f\" % (i,t) for i,t in enumerate(text_data['train_test'][0:100]) if t>TRAIN_PCT] )\n",
    "print( ', '.join([ \"%d\" % (i) for i,t in enumerate(text_data['train_test'][0:200]) if t>TRAIN_PCT] ), \"\\n\")\n",
    "\n",
    "model_test(model, 70)  # Uninitialised model is *terrible*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    batch_gen = caption_training_batch_generator(emb_input, emb_output, batch_size=BATCH_SIZE)\n",
    "    X,Y = next(batch_gen)\n",
    "\n",
    "    model.train_on_batch(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights_filename = './data/cache/%s_%s_%s_%%04d.h5' % (model.name, emb_input.name, emb_output.name)\n",
    "weights_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "epoch = 50   # Force a specific epoch\n",
    "while os.path.isfile(weights_filename % (epoch,)):\n",
    "    epoch += 1\n",
    "\n",
    "if epoch>0:  # i.e. we found something\n",
    "    model.load_weights(weights_filename % (epoch-1,))  # Go back one step\n",
    "    print(\"Loaded weights from previously saved epoch %d\" % (epoch-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    model.fit_generator(caption_training_batch_generator(emb_input, emb_output, batch_size=BATCH_SIZE), \n",
    "                        len(caption_arr_train)/BATCH_SIZE, epochs=epoch+1, initial_epoch=epoch)\n",
    "    model.save_weights(weights_filename % (epoch,))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#raise(\"Intentional error to stop execution flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test the current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_test(model, 2170)  # 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def bleu_score(model, img_idx, img_path='./data/Flickr30k/flickr30k-images/'):\n",
    "    img_name = text_data['img_arr'][img_idx]\n",
    "    captions_real = text_data['img_to_captions'][img_name]\n",
    "\n",
    "    features = feature_arr[ image_feature_idx[img_name] ] \n",
    "    for _ in range(5):\n",
    "        caption, prob = sample_caption(model, features)\n",
    "        score = nltk.translate.bleu_score.sentence_bleu(\n",
    "            [ c.split(' ') for c in captions_real ], \n",
    "            caption.split(' '),\n",
    "        )\n",
    "        print(\"  * %.2f : %s\" % (score, caption,))\n",
    "    \n",
    "bleu_score(model, 70)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}