{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Purposing a Pretrained Network\n",
    "\n",
    "Since a large CNN is very time-consuming to train (even on a GPU), and requires huge amounts of data, is there any way to use a pre-calculated one instead of retraining the whole thing from scratch?\n",
    "\n",
    "This notebook shows how this can be done.  And it works surprisingly well.\n",
    "\n",
    "\n",
    "##  How do we classify images with untrained classes?\n",
    "\n",
    "This notebook extracts a vector representation of a set of images using the GoogLeNet CNN pretrained on ImageNet.  It then builds a 'simple SVM classifier', allowing new images can be classified directly.  No retraining of the original CNN is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "from urllib.request import urlopen  # Python 3+ version (instead of urllib2)\n",
    "\n",
    "CLASS_DIR='./images/cars'\n",
    "#CLASS_DIR='./images/seefood'  # for HotDog vs NotHotDog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add TensorFlow Slim Model Zoo to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "slim_models_dir = './models/tensorflow_zoo'\n",
    "\n",
    "if not os.path.exists(slim_models_dir):\n",
    "    print(\"Creating model/tensorflow_zoo directory\")\n",
    "    os.makedirs(slim_models_dir)\n",
    "if not os.path.isfile( os.path.join(slim_models_dir, 'models', 'README.md') ):\n",
    "    print(\"Cloning tensorflow model zoo under %s\" % (slim_models_dir, ))\n",
    "    !cd {slim_models_dir}; git clone https://github.com/tensorflow/models.git\n",
    "\n",
    "sys.path.append(slim_models_dir + \"/models/slim\")\n",
    "\n",
    "print(\"Model Zoo model code installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Inception v1 (GoogLeNet) Architecture|\n",
    "\n",
    "![GoogLeNet Architecture](../../images/presentation/googlenet-arch_1228x573.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Inception V1 checkpoint\u00b6\n",
    "\n",
    "Functions for building the GoogLeNet model with TensorFlow / slim and preprocessing the images are defined in ```model.inception_v1_tf``` - which was downloaded from the TensorFlow / slim [Model Zoo](https://github.com/tensorflow/models/tree/master/slim).\n",
    "\n",
    "The actual code for the ```slim``` model will be <a href=\"model/tensorflow_zoo/models/slim/nets/inception_v1.py\" target=_blank>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import dataset_utils\n",
    "\n",
    "targz = \"inception_v1_2016_08_28.tar.gz\"\n",
    "url = \"http://download.tensorflow.org/models/\"+targz\n",
    "checkpoints_dir = './data/tensorflow_zoo/checkpoints'\n",
    "\n",
    "if not os.path.exists(checkpoints_dir):\n",
    "    os.makedirs(checkpoints_dir)\n",
    "\n",
    "if not os.path.isfile( os.path.join(checkpoints_dir, 'inception_v1.ckpt') ):\n",
    "    tarfilepath = os.path.join(checkpoints_dir, targz)\n",
    "    if os.path.isfile(tarfilepath):\n",
    "        import tarfile\n",
    "        tarfile.open(tarfilepath, 'r:gz').extractall(checkpoints_dir)\n",
    "    else:\n",
    "        dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)\n",
    "        \n",
    "    # Get rid of tarfile source (the checkpoint itself will remain)\n",
    "    os.unlink(tarfilepath)\n",
    "        \n",
    "print(\"Checkpoint available locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model and select layers we need - the features are taken from the final network layer, before the softmax nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "from nets import inception\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imagenet_labels_file = './data/imagenet_synset_words.txt'\n",
    "if os.path.isfile(imagenet_labels_file):\n",
    "    print(\"Loading ImageNet synset data locally\")\n",
    "    with open(imagenet_labels_file, 'r') as f:\n",
    "        imagenet_labels = {0: 'background'}\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            # n01440764 tench, Tinca tinca\n",
    "            synset,human = line.strip().split(' ', 1)\n",
    "            imagenet_labels[ i+1 ] = human\n",
    "\n",
    "else:\n",
    "    print(\"Downloading ImageNet synset data from repo\")\n",
    "    from datasets import imagenet\n",
    "    imagenet_labels = imagenet.create_readable_names_for_imagenet_labels()\n",
    "    \n",
    "print(\"ImageNet synset labels available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# This creates an image 'placeholder'\n",
    "# input_image = tf.image.decode_jpeg(image_string, channels=3)    \n",
    "input_image = tf.placeholder(tf.uint8, shape=[None, None, 3], name='input_image')\n",
    "\n",
    "# Define the pre-processing chain within the graph - based on the input 'image' above\n",
    "processed_image = inception_preprocessing.preprocess_image(input_image, image_size, image_size, is_training=False)\n",
    "processed_images = tf.expand_dims(processed_image, 0)\n",
    "\n",
    "# Reverse out some of the transforms, so we can see the area/scaling of the inception input\n",
    "numpyish_image = tf.multiply(processed_image, 0.5)\n",
    "numpyish_image = tf.add(numpyish_image, 0.5)\n",
    "numpyish_image = tf.multiply(numpyish_image, 255.0)\n",
    "\n",
    "# Create the model - which uses the above pre-processing on image\n",
    "#   it also uses the default arg scope to configure the batch norm parameters.\n",
    "print(\"Model builder starting\")\n",
    "\n",
    "# Here is the actual model zoo model being instantiated :\n",
    "with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "    logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False)\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "# Create an operation that loads the pre-trained model from the checkpoint\n",
    "init_fn = slim.assign_from_checkpoint_fn(\n",
    "    os.path.join(checkpoints_dir, 'inception_v1.ckpt'),\n",
    "    slim.get_model_variables('InceptionV1')\n",
    ")\n",
    "\n",
    "print(\"Model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the network layout graph on TensorBoard\n",
    "\n",
    "This isn't very informative, since the inception graph is pretty complex..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#writer = tf.summary.FileWriter(logdir='../tensorflow.logdir/', graph=tf.get_default_graph())\n",
    "#writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an Example Image\n",
    "\n",
    "Pull in an image into a numpy object :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Read from the Web\n",
    "    from io import BytesIO \n",
    "    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'\n",
    "    image_string = urlopen(url).read()\n",
    "    im = plt.imread(BytesIO(image_string), format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Read from a file via a queue ==> brain damage in jupyter\n",
    "    \n",
    "    #filename_queue = tf.train.string_input_producer( tf.train.match_filenames_once(\"./images/*.jpg\") )\n",
    "    filename_queue = tf.train.string_input_producer( ['./images/cat-with-tongue_224x224.jpg'] )\n",
    "    #_ = filename_queue.dequeue()  # Ditch the first value\n",
    "    \n",
    "    image_reader = tf.WholeFileReader()\n",
    "    \n",
    "    _, image_string = image_reader.read(filename_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read from a file\n",
    "im = plt.imread(\"./images/cat-with-tongue_224x224.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(im.shape, im[0,0])   # (height, width, channels), (uint8, uint8, uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def crop_middle_square_area(np_image):\n",
    "    h, w, _ = np_image.shape\n",
    "    h = int(h/2)\n",
    "    w = int(w/2)\n",
    "    if h>w:\n",
    "        return np_image[ h-w:h+w, : ]\n",
    "    return np_image[ :, w-h:w+h ]    \n",
    "im_sq = crop_middle_square_area(im)\n",
    "im_sq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run using the Example Image\n",
    "\n",
    "Let's verify that GoogLeNet / Inception-v1 and our preprocessing are functioning properly :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's run the pre-trained model\n",
    "with tf.Session() as sess:\n",
    "    # This is the loader 'op' we defined above\n",
    "    init_fn(sess)  \n",
    "    \n",
    "    # This is two ops : one merely loads the image from numpy, \n",
    "    #   the other runs the network to get the class probabilities\n",
    "    np_image, np_probs = sess.run([numpyish_image, probabilities], feed_dict={input_image:im_sq})\n",
    "        \n",
    "# These are regular numpy operations\n",
    "probs = np_probs[0, :]\n",
    "sorted_inds = [i[0] for i in sorted(enumerate(-probs), key=lambda x:x[1])]\n",
    "\n",
    "# And now plot out the results\n",
    "plt.figure()\n",
    "plt.imshow(np_image.astype(np.uint8))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "for i in range(5):\n",
    "    index = sorted_inds[i]\n",
    "    print('Probability %0.2f%% => [%s]' % (probs[index], imagenet_labels[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Use the Network to create 'features' for the training images\n",
    "\n",
    "Now go through the input images and feature-ize them at the 'logit level' according to the pretrained network.\n",
    "\n",
    "<!-- [Logits vs the softmax probabilities](images/presentation/softmax-layer-generic_676x327.png) !-->\n",
    "\n",
    "![Network Picture](images/presentation/commerce-network_631x540.png)\n",
    "\n",
    "NB: The pretraining was done on ImageNet - there wasn't anything specific to the recognition task we're doing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "classes = sorted( [ d for d in os.listdir(CLASS_DIR) if os.path.isdir(\"%s/%s\" % (CLASS_DIR, d)) ] )\n",
    "classes # Sorted for for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = dict(filepath=[], features=[], target=[])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # This is the loader 'op' we defined above\n",
    "    init_fn(sess)  \n",
    "    print(\"Loaded pre-trained model\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for class_i, directory in enumerate(classes):\n",
    "        for filename in os.listdir(\"%s/%s\" % (CLASS_DIR, directory, )):\n",
    "            filepath = '%s/%s/%s' % (CLASS_DIR, directory, filename, )\n",
    "            if os.path.isdir(filepath): continue\n",
    "                \n",
    "            im = plt.imread(filepath)\n",
    "            im_sq = crop_middle_square_area(im)\n",
    "\n",
    "            # This is two ops : one merely loads the image from numpy, \n",
    "            #   the other runs the network to get the 'logit features'\n",
    "            rawim, np_logits = sess.run([numpyish_image, logits], feed_dict={input_image:im_sq})\n",
    "    \n",
    "            train['filepath'].append(filepath)\n",
    "            train['features'].append(np_logits[0])\n",
    "            train['target'].append( class_i )\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(rawim.astype('uint8'))\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.text(320, 50, '{}'.format(filename), fontsize=14)\n",
    "            plt.text(320, 80, 'Train as class \"{}\"'.format(directory), fontsize=12)\n",
    "    \n",
    "print(\"DONE : %6.2f seconds each\" %(float(time.time() - t0)/len(train),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build an SVM model over the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train['features'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(train['features'], train['target']) # learn from the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Use the SVM model to classify the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_image_files = [f for f in os.listdir(CLASS_DIR) if not os.path.isdir(\"%s/%s\" % (CLASS_DIR, f))]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # This is the loader 'op' we defined above\n",
    "    init_fn(sess)  \n",
    "    print(\"Loaded pre-trained model\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for filename in sorted(test_image_files):\n",
    "        im = plt.imread('%s/%s' % (CLASS_DIR,filename,))\n",
    "        im_sq = crop_middle_square_area(im)\n",
    "\n",
    "        # This is two ops : one merely loads the image from numpy, \n",
    "        #   the other runs the network to get the class probabilities\n",
    "        rawim, np_logits = sess.run([numpyish_image, logits], feed_dict={input_image:im_sq})\n",
    "\n",
    "        prediction_i = classifier.predict([ np_logits[0] ])\n",
    "        decision     = classifier.decision_function([ np_logits[0] ])\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(rawim.astype('uint8'))\n",
    "        plt.axis('off')\n",
    "\n",
    "        prediction = classes[ prediction_i[0] ]\n",
    "\n",
    "        plt.text(350, 50, '{} : Distance from boundary = {:5.2f}'.format(prediction, decision[0]), fontsize=20)\n",
    "        plt.text(350, 75, '{}'.format(filename), fontsize=14)\n",
    "    \n",
    "print(\"DONE : %6.2f seconds each\" %(float(time.time() - t0)/len(test_image_files),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Exercise : Try your own ideas\n",
    "\n",
    "The whole training regime here is based on the way the image directories are structured.  So building your own example shouldn't be very difficult.\n",
    "\n",
    "Suppose you wanted to classify pianos into Upright and Grand : \n",
    "\n",
    "*  Create a ```pianos``` directory and point the ```CLASS_DIR``` variable at it\n",
    "*  Within the ```pianos``` directory, create subdirectories for each of the classes (i.e. ```Upright``` and ```Grand```).  The directory names will be used as the class labels\n",
    "*  Inside the class directories, put a 'bunch' of positive examples of the respective classes - these can be images in any reasonable format, of any size (no smaller than 224x224).\n",
    "   +  The images will be automatically resized so that their smallest dimension is 224, and then a square 'crop' area taken from their centers (since ImageNet networks are typically tuned to answering on 224x224 images)\n",
    "*  Test images should be put in the ```pianos``` directory itelf (which is logical, since we don't *know* their classes yet)\n",
    "\n",
    "Finally, re-run everything - checking that the training images are read in correctly, that there are no errors along the way, and that (finally) the class predictions on the test set come out as expected.\n",
    "\n",
    "If/when it works - please let everyone know : We can add that as an example for next time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}